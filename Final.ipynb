{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import torchattacks\n",
    "from torchattacks import CW\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Creating dataset using torch dataloaders\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))]) # Normalizing dataset\n",
    "\n",
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# Initialize GPU\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model structure\n",
    "class HNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(HNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1) # add this because I need one-hot label and MSE loss\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SubModel Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submodel Structure for training residual\n",
    "class NHNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NHNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1) # add this because I need one-hot label and MSE loss\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some data structures to store useful data\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Parameters, optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial model\n",
    "initial_model = HNet()\n",
    "\n",
    "#Create the optimizer for the initial model\n",
    "optimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "# Create Loss function for the intial model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#Change model into cuda mode\n",
    "if torch.cuda.is_available():\n",
    "    initial_model = initial_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function for Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model Training Function\n",
    "def train(epoch):\n",
    "    initial_model.train()\n",
    "#     exp_lr_scheduler.step()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = initial_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        # torch.save(initial_model.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/model.pth')\n",
    "        # torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/optimizer.pth')\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function for Intial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Model Evaluating Function\n",
    "def evaluate(data_loader):\n",
    "    initial_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "        \n",
    "            output = initial_model(data)\n",
    "        \n",
    "            loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    test_losses.append(loss)    \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training the Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.602848\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 1.549241\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 1.581888\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 1.536988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cozyn\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 1.5576, Accuracy: 54379/60000 (90.632%)\n",
      "\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.562668\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.532746\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.530872\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.574295\n",
      "\n",
      "Average loss: 1.5313, Accuracy: 55880/60000 (93.133%)\n",
      "\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 1.515534\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 1.524624\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.525929\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 1.502936\n",
      "\n",
      "Average loss: 1.5157, Accuracy: 56773/60000 (94.622%)\n",
      "\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 1.501767\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 1.551804\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 1.538258\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.534766\n",
      "\n",
      "Average loss: 1.5180, Accuracy: 56624/60000 (94.373%)\n",
      "\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.514447\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.528231\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.516948\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 1.529155\n",
      "\n",
      "Average loss: 1.5074, Accuracy: 57218/60000 (95.363%)\n",
      "\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.514500\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.528666\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 1.496560\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 1.501138\n",
      "\n",
      "Average loss: 1.5130, Accuracy: 56874/60000 (94.790%)\n",
      "\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 1.530809\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 1.539706\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 1.527191\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 1.521303\n",
      "\n",
      "Average loss: 1.5114, Accuracy: 56975/60000 (94.958%)\n",
      "\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 1.531494\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.490708\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 1.500590\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 1.539338\n",
      "\n",
      "Average loss: 1.5035, Accuracy: 57456/60000 (95.760%)\n",
      "\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.525730\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.513473\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.500546\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 1.511177\n",
      "\n",
      "Average loss: 1.5338, Accuracy: 55620/60000 (92.700%)\n",
      "\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 1.530999\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.550530\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.526278\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.586829\n",
      "\n",
      "Average loss: 1.5091, Accuracy: 57117/60000 (95.195%)\n",
      "\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.494136\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.563535\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.482316\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.484455\n",
      "\n",
      "Average loss: 1.5123, Accuracy: 56931/60000 (94.885%)\n",
      "\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 1.493333\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 1.538868\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 1.539934\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 1.500013\n",
      "\n",
      "Average loss: 1.5031, Accuracy: 57474/60000 (95.790%)\n",
      "\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 1.515614\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 1.556699\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 1.492432\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 1.486581\n",
      "\n",
      "Average loss: 1.5046, Accuracy: 57395/60000 (95.658%)\n",
      "\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 1.508024\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 1.493280\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 1.547075\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 1.482033\n",
      "\n",
      "Average loss: 1.5071, Accuracy: 57235/60000 (95.392%)\n",
      "\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 1.504057\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 1.513875\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 1.508353\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 1.554730\n",
      "\n",
      "Average loss: 1.5061, Accuracy: 57281/60000 (95.468%)\n",
      "\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 1.479192\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 1.476774\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 1.517881\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 1.516349\n",
      "\n",
      "Average loss: 1.5019, Accuracy: 57547/60000 (95.912%)\n",
      "\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 1.556511\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 1.547364\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 1.492128\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 1.501955\n",
      "\n",
      "Average loss: 1.5007, Accuracy: 57609/60000 (96.015%)\n",
      "\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 1.519263\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 1.523810\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 1.522984\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 1.539315\n",
      "\n",
      "Average loss: 1.5057, Accuracy: 57316/60000 (95.527%)\n",
      "\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 1.510135\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 1.492400\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 1.531136\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 1.507694\n",
      "\n",
      "Average loss: 1.5102, Accuracy: 57047/60000 (95.078%)\n",
      "\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 1.538876\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 1.492112\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 1.509677\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 1.499965\n",
      "\n",
      "Average loss: 1.5087, Accuracy: 57143/60000 (95.238%)\n",
      "\n",
      "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 1.510746\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 1.507831\n",
      "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 1.501232\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 1.492792\n",
      "\n",
      "Average loss: 1.5586, Accuracy: 54122/60000 (90.203%)\n",
      "\n",
      "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 1.512181\n",
      "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 1.500241\n",
      "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 1.515822\n",
      "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 1.527208\n",
      "\n",
      "Average loss: 1.5118, Accuracy: 56964/60000 (94.940%)\n",
      "\n",
      "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 1.507831\n",
      "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 1.503281\n",
      "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 1.526710\n",
      "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 1.503437\n",
      "\n",
      "Average loss: 1.5001, Accuracy: 57658/60000 (96.097%)\n",
      "\n",
      "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 1.520601\n",
      "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 1.503442\n",
      "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 1.496925\n",
      "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 1.484596\n",
      "\n",
      "Average loss: 1.5133, Accuracy: 56861/60000 (94.768%)\n",
      "\n",
      "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 1.492218\n",
      "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 1.523609\n",
      "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 1.492442\n",
      "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 1.508051\n",
      "\n",
      "Average loss: 1.5085, Accuracy: 57151/60000 (95.252%)\n",
      "\n",
      "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 1.468963\n",
      "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 1.484505\n",
      "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 1.500292\n",
      "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 1.507941\n",
      "\n",
      "Average loss: 1.5262, Accuracy: 56088/60000 (93.480%)\n",
      "\n",
      "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 1.562744\n",
      "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 1.539270\n",
      "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 1.484590\n",
      "\n",
      "Average loss: 1.5002, Accuracy: 57657/60000 (96.095%)\n",
      "\n",
      "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 1.502919\n",
      "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 1.510360\n",
      "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 1.547134\n",
      "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 1.488748\n",
      "\n",
      "Average loss: 1.5030, Accuracy: 57486/60000 (95.810%)\n",
      "\n",
      "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 1.484757\n",
      "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 1.577652\n",
      "\n",
      "Average loss: 1.5019, Accuracy: 57547/60000 (95.912%)\n",
      "\n",
      "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 1.493515\n",
      "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 1.511580\n",
      "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 1.516100\n",
      "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 1.515550\n",
      "\n",
      "Average loss: 1.5109, Accuracy: 57011/60000 (95.018%)\n",
      "\n",
      "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 1.515706\n",
      "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 1.476825\n",
      "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 1.500118\n",
      "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 1.476776\n",
      "\n",
      "Average loss: 1.5289, Accuracy: 55927/60000 (93.212%)\n",
      "\n",
      "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 1.492405\n",
      "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 1.523669\n",
      "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 1.523663\n",
      "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 1.494959\n",
      "\n",
      "Average loss: 1.5042, Accuracy: 57407/60000 (95.678%)\n",
      "\n",
      "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 1.518426\n",
      "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 1.492401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 1.508222\n",
      "\n",
      "Average loss: 1.5023, Accuracy: 57525/60000 (95.875%)\n",
      "\n",
      "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 1.508074\n",
      "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 1.547074\n",
      "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 1.548809\n",
      "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 1.521500\n",
      "\n",
      "Average loss: 1.5059, Accuracy: 57314/60000 (95.523%)\n",
      "\n",
      "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 1.484142\n",
      "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 1.492401\n",
      "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 1.523629\n",
      "\n",
      "Average loss: 1.5080, Accuracy: 57187/60000 (95.312%)\n",
      "\n",
      "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 1.507460\n",
      "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 1.491980\n",
      "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 1.514934\n",
      "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 1.492401\n",
      "\n",
      "Average loss: 1.5127, Accuracy: 56892/60000 (94.820%)\n",
      "\n",
      "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 1.484588\n",
      "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 1.484589\n",
      "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 1.522655\n",
      "\n",
      "Average loss: 1.5122, Accuracy: 56936/60000 (94.893%)\n",
      "\n",
      "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 1.516813\n",
      "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 1.500213\n",
      "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 1.507856\n",
      "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 1.539276\n",
      "\n",
      "Average loss: 1.5048, Accuracy: 57379/60000 (95.632%)\n",
      "\n",
      "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 1.492907\n",
      "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 1.492535\n",
      "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 1.476776\n",
      "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 1.509126\n",
      "\n",
      "Average loss: 1.5127, Accuracy: 56906/60000 (94.843%)\n",
      "\n",
      "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 1.492027\n",
      "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 1.518802\n",
      "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 1.515162\n",
      "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 1.556042\n",
      "\n",
      "Average loss: 1.5021, Accuracy: 57543/60000 (95.905%)\n",
      "\n",
      "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 1.468964\n",
      "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 1.547027\n",
      "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 1.505101\n",
      "\n",
      "Average loss: 1.5064, Accuracy: 57278/60000 (95.463%)\n",
      "\n",
      "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 1.511928\n",
      "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 1.530313\n",
      "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 1.546970\n",
      "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 1.492369\n",
      "\n",
      "Average loss: 1.5015, Accuracy: 57568/60000 (95.947%)\n",
      "\n",
      "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 1.547435\n",
      "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 1.545237\n",
      "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 1.515838\n",
      "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 1.508191\n",
      "\n",
      "Average loss: 1.5219, Accuracy: 56351/60000 (93.918%)\n",
      "\n",
      "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 1.523651\n",
      "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 1.501332\n",
      "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 1.484589\n",
      "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 1.500210\n",
      "\n",
      "Average loss: 1.5026, Accuracy: 57512/60000 (95.853%)\n",
      "\n",
      "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 1.554901\n",
      "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 1.509511\n",
      "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 1.515838\n",
      "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 1.562711\n",
      "\n",
      "Average loss: 1.5174, Accuracy: 56618/60000 (94.363%)\n",
      "\n",
      "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 1.486054\n",
      "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 1.499688\n",
      "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 1.562706\n",
      "\n",
      "Average loss: 1.5228, Accuracy: 56298/60000 (93.830%)\n",
      "\n",
      "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 1.546576\n",
      "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 1.523651\n",
      "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 1.492431\n",
      "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 1.515838\n",
      "\n",
      "Average loss: 1.5119, Accuracy: 56951/60000 (94.918%)\n",
      "\n",
      "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 1.461151\n",
      "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 1.492401\n",
      "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 1.531463\n",
      "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 1.507985\n",
      "\n",
      "Average loss: 1.5050, Accuracy: 57371/60000 (95.618%)\n",
      "\n",
      "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 1.500183\n",
      "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 1.523365\n",
      "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 1.492401\n",
      "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 1.500206\n",
      "\n",
      "Average loss: 1.5210, Accuracy: 56408/60000 (94.013%)\n",
      "\n",
      "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 1.469062\n",
      "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 1.484471\n",
      "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 1.480292\n",
      "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "\n",
      "Average loss: 1.5022, Accuracy: 57532/60000 (95.887%)\n",
      "\n",
      "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 1.547088\n",
      "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 1.515838\n",
      "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 1.484588\n",
      "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 1.569695\n",
      "\n",
      "Average loss: 1.5389, Accuracy: 55326/60000 (92.210%)\n",
      "\n",
      "Train Epoch: 51 [12800/60000 (21%)]\tLoss: 1.468963\n",
      "Train Epoch: 51 [25600/60000 (43%)]\tLoss: 1.508005\n",
      "Train Epoch: 51 [38400/60000 (64%)]\tLoss: 1.484588\n",
      "Train Epoch: 51 [51200/60000 (85%)]\tLoss: 1.483929\n",
      "\n",
      "Average loss: 1.4983, Accuracy: 57773/60000 (96.288%)\n",
      "\n",
      "Train Epoch: 52 [12800/60000 (21%)]\tLoss: 1.515857\n",
      "Train Epoch: 52 [25600/60000 (43%)]\tLoss: 1.539604\n",
      "Train Epoch: 52 [38400/60000 (64%)]\tLoss: 1.537979\n",
      "Train Epoch: 52 [51200/60000 (85%)]\tLoss: 1.508027\n",
      "\n",
      "Average loss: 1.5133, Accuracy: 56871/60000 (94.785%)\n",
      "\n",
      "Train Epoch: 53 [12800/60000 (21%)]\tLoss: 1.557441\n",
      "Train Epoch: 53 [25600/60000 (43%)]\tLoss: 1.508026\n",
      "Train Epoch: 53 [38400/60000 (64%)]\tLoss: 1.503356\n",
      "Train Epoch: 53 [51200/60000 (85%)]\tLoss: 1.508026\n",
      "\n",
      "Average loss: 1.5284, Accuracy: 55964/60000 (93.273%)\n",
      "\n",
      "Train Epoch: 54 [12800/60000 (21%)]\tLoss: 1.538777\n",
      "Train Epoch: 54 [25600/60000 (43%)]\tLoss: 1.586151\n",
      "Train Epoch: 54 [38400/60000 (64%)]\tLoss: 1.524225\n",
      "Train Epoch: 54 [51200/60000 (85%)]\tLoss: 1.492317\n",
      "\n",
      "Average loss: 1.5103, Accuracy: 57043/60000 (95.072%)\n",
      "\n",
      "Train Epoch: 55 [12800/60000 (21%)]\tLoss: 1.500212\n",
      "Train Epoch: 55 [25600/60000 (43%)]\tLoss: 1.508025\n",
      "Train Epoch: 55 [38400/60000 (64%)]\tLoss: 1.517406\n",
      "Train Epoch: 55 [51200/60000 (85%)]\tLoss: 1.500211\n",
      "\n",
      "Average loss: 1.5511, Accuracy: 54597/60000 (90.995%)\n",
      "\n",
      "Train Epoch: 56 [12800/60000 (21%)]\tLoss: 1.539234\n",
      "Train Epoch: 56 [25600/60000 (43%)]\tLoss: 1.502225\n",
      "Train Epoch: 56 [38400/60000 (64%)]\tLoss: 1.539276\n",
      "Train Epoch: 56 [51200/60000 (85%)]\tLoss: 1.539292\n",
      "\n",
      "Average loss: 1.5079, Accuracy: 57196/60000 (95.327%)\n",
      "\n",
      "Train Epoch: 57 [12800/60000 (21%)]\tLoss: 1.476776\n",
      "Train Epoch: 57 [25600/60000 (43%)]\tLoss: 1.476776\n",
      "Train Epoch: 57 [38400/60000 (64%)]\tLoss: 1.500224\n",
      "Train Epoch: 57 [51200/60000 (85%)]\tLoss: 1.500208\n",
      "\n",
      "Average loss: 1.5344, Accuracy: 55604/60000 (92.673%)\n",
      "\n",
      "Train Epoch: 58 [12800/60000 (21%)]\tLoss: 1.514438\n",
      "Train Epoch: 58 [25600/60000 (43%)]\tLoss: 1.531466\n",
      "Train Epoch: 58 [38400/60000 (64%)]\tLoss: 1.508020\n",
      "Train Epoch: 58 [51200/60000 (85%)]\tLoss: 1.492242\n",
      "\n",
      "Average loss: 1.5033, Accuracy: 57466/60000 (95.777%)\n",
      "\n",
      "Train Epoch: 59 [12800/60000 (21%)]\tLoss: 1.487408\n",
      "Train Epoch: 59 [25600/60000 (43%)]\tLoss: 1.476776\n",
      "Train Epoch: 59 [38400/60000 (64%)]\tLoss: 1.515839\n",
      "Train Epoch: 59 [51200/60000 (85%)]\tLoss: 1.515838\n",
      "\n",
      "Average loss: 1.5014, Accuracy: 57586/60000 (95.977%)\n",
      "\n",
      "Train Epoch: 60 [12800/60000 (21%)]\tLoss: 1.484588\n",
      "Train Epoch: 60 [25600/60000 (43%)]\tLoss: 1.484588\n",
      "Train Epoch: 60 [38400/60000 (64%)]\tLoss: 1.501272\n",
      "Train Epoch: 60 [51200/60000 (85%)]\tLoss: 1.523651\n",
      "\n",
      "Average loss: 1.4992, Accuracy: 57716/60000 (96.193%)\n",
      "\n",
      "Train Epoch: 61 [12800/60000 (21%)]\tLoss: 1.508026\n",
      "Train Epoch: 61 [25600/60000 (43%)]\tLoss: 1.523495\n",
      "Train Epoch: 61 [38400/60000 (64%)]\tLoss: 1.513451\n",
      "Train Epoch: 61 [51200/60000 (85%)]\tLoss: 1.541860\n",
      "\n",
      "Average loss: 1.4997, Accuracy: 57684/60000 (96.140%)\n",
      "\n",
      "Train Epoch: 62 [12800/60000 (21%)]\tLoss: 1.578316\n",
      "Train Epoch: 62 [25600/60000 (43%)]\tLoss: 1.547088\n",
      "Train Epoch: 62 [38400/60000 (64%)]\tLoss: 1.523652\n",
      "Train Epoch: 62 [51200/60000 (85%)]\tLoss: 1.562757\n",
      "\n",
      "Average loss: 1.5213, Accuracy: 56384/60000 (93.973%)\n",
      "\n",
      "Train Epoch: 63 [12800/60000 (21%)]\tLoss: 1.539276\n",
      "Train Epoch: 63 [25600/60000 (43%)]\tLoss: 1.524761\n",
      "Train Epoch: 63 [38400/60000 (64%)]\tLoss: 1.484785\n",
      "Train Epoch: 63 [51200/60000 (85%)]\tLoss: 1.500181\n",
      "\n",
      "Average loss: 1.5238, Accuracy: 56238/60000 (93.730%)\n",
      "\n",
      "Train Epoch: 64 [12800/60000 (21%)]\tLoss: 1.492401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 64 [25600/60000 (43%)]\tLoss: 1.500212\n",
      "Train Epoch: 64 [38400/60000 (64%)]\tLoss: 1.523651\n",
      "Train Epoch: 64 [51200/60000 (85%)]\tLoss: 1.518947\n",
      "\n",
      "Average loss: 1.5082, Accuracy: 57166/60000 (95.277%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "n_epochs = 65\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Load the Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this model so that I dont have to train again in the future\n",
    "torch.save(initial_model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HNet(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model from local file\n",
    "initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculatee mse residual and normalize the result to values between 0 ~ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate mse residual based on the wiki\n",
    "def mseresidual(y, F):\n",
    "    residual = y - F\n",
    "    absolute = torch.abs(residual)\n",
    "    residual = residual / torch.max(absolute)\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Code for gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function used to train and find optimal gamma for submodels\n",
    "#input: intial model that's already trained\n",
    "#M is number of submodels needed to be trained\n",
    "def GradientBoosting(initial_model, M):\n",
    "    gamma_exp = torch.ones([M], dtype = torch.float64) # used to hold the final optimized gamma\n",
    "    models = [] # used to hold all the models\n",
    "    residual_list = [] # used to hold the residual of each batch calculated\n",
    "    for m in range(M):\n",
    "        # Intialize submodels\n",
    "        Hmodel = NHNet()\n",
    "        Hcriterion = nn.MSELoss()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp = gamma_exp.cuda()\n",
    "            Hcriterion = Hcriterion.cuda()\n",
    "            \n",
    "        # Start Training\n",
    "        epoch = 40\n",
    "        Hoptimizer = optim.Adam(Hmodel.parameters(), lr=0.001)\n",
    "        for i in range(epoch):\n",
    "            Hmodel.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                # Create one-hot label target tensor\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                # Calculate F(x)\n",
    "                output = initial_model(data)\n",
    "                # Calculate the output from all the models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        output = output.cuda()\n",
    "                        model = model.cuda()\n",
    "                    output = output + gamma_exp[j] * model(data)\n",
    "#                 print(\"output is:\", output)\n",
    "                #Convert into Onehot label so that it would be able to calculate the residual\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                #Calculate Residual\n",
    "#                 print(\"target_onehot is:\", target_onehot)\n",
    "#                 print(\"output is:\", output)\n",
    "                residual = mseresidual(target_onehot, output)\n",
    "                houtput = Hmodel(data)\n",
    "#                 print(\"houtput is:\", houtput)\n",
    "                residual = residual.type(torch.cuda.FloatTensor)\n",
    "                houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                #Calculate the loss\n",
    "                loss = Hcriterion(houtput, residual)\n",
    "                Hoptimizer.zero_grad()\n",
    "                loss.backward(retain_graph = True)\n",
    "                Hoptimizer.step()\n",
    "                # Print out current Process\n",
    "                if (batch_idx + 1)% 100 == 0 and i % 10 == 0:\n",
    "                    print('Train Epoch: Model Number: {} {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                            m+1,i, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                            100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "        models.append(Hmodel)\n",
    "        \n",
    "        # Initialize a random gamma\n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.01)\n",
    "        Gcriterion = nn.MSELoss()\n",
    "        # Start finding the best gamma\n",
    "        for i in range(20):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                    \n",
    "                #Calculate the initial output\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                #Calculate the final output by combining all previous models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp[j]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "                # Covert into one-hot label\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                # Get the currect model output\n",
    "                temp = Hmodel(data)\n",
    "                # Find the current ensemble model output\n",
    "                predicted = output + gamma * temp\n",
    "                # Calculate the loss\n",
    "                loss = Gcriterion(predicted, target_onehot)\n",
    "                loss.backward(retain_graph = True)\n",
    "                # Optimize the gamma\n",
    "                Goptimizer.step()  \n",
    "        gamma_exp[m] = gamma\n",
    "    print(gamma_exp)\n",
    "    return models, gamma_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Diversity Promoting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_offset = 1e-20\n",
    "det_offset = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADPGradient(initial_model, M):\n",
    "    gamma_exp_adp = torch.ones([M], dtype = torch.float64)\n",
    "    models_adp = [] # used to hold all the models\n",
    "    residual_list = [] # used to hold the residual of each batch calculated\n",
    "    epoch = 30\n",
    "    writer = SummaryWriter()\n",
    "    for m in range(M):\n",
    "        # Intialize submodels\n",
    "        Hmodel = NHNet()\n",
    "        Hcriterion = nn.MSELoss()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp_adp = gamma_exp_adp.cuda()\n",
    "            Hcriterion = Hcriterion.cuda()\n",
    "            params = []\n",
    "            params += list(Hmodel.parameters())\n",
    "            for j in models_adp:\n",
    "                params += list(j.parameters())\n",
    "            Hoptimizer = optim.Adam(params, lr=0.001, weight_decay=1e-4, eps=1e-7)\n",
    "        for i in range(epoch):\n",
    "            Hmodel.train()\n",
    "            for j in models_adp:\n",
    "                j.train()\n",
    "                \n",
    "            losses = 0\n",
    "            ce_losses = 0\n",
    "            ee_losses = 0\n",
    "            det_losses = 0\n",
    "            alpha = 2.0\n",
    "            beta = 0.5\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                y_true = torch.zeros(data.size(0), nb_digits).cuda()\n",
    "                y_true.scatter_(1, target.view(-1,1), 1)\n",
    "                ce_loss = 0\n",
    "                mask_non_y_pred = []\n",
    "                ensemble_probs = 0\n",
    "                for k in range(m+1):\n",
    "                    output = initial_model(data)\n",
    "                    target = target.view(-1,1)\n",
    "                    target_onehot.zero_()\n",
    "                    target_onehot.scatter_(1, target, 1)\n",
    "                    for j in range(m):\n",
    "                        model = models_adp[j]\n",
    "                        if torch.cuda.is_available():\n",
    "                            model = model.cuda()\n",
    "                            output = output.cuda()\n",
    "                            gamma_temp = gamma_exp_adp[j]\n",
    "                            gamma_temp = gamma_temp.cuda()\n",
    "                        if k < m and j == k:\n",
    "                            residual = mseresidual(target_onehot, output)\n",
    "                            houtput = model(data)\n",
    "                            residual = residual.type(torch.cuda.FloatTensor)\n",
    "                            houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                            ce_loss += Hcriterion(houtput, residual)\n",
    "                            y_pred = F.softmax(output, dim=-1)\n",
    "#                             y_pred = torch.reshape(output,dim=-1)\n",
    "                            bool_R_y_true = torch.eq(torch.ones_like(y_true) - y_true, torch.ones_like(y_true))\n",
    "                            mask_non_y_pred.append(torch.masked_select(y_pred, bool_R_y_true).reshape(-1, nb_digits-1))\n",
    "                            ensemble_probs += y_pred\n",
    "                        output = output + gamma_temp * model(data)\n",
    "                    \n",
    "                    if k == m:\n",
    "                        residual = mseresidual(target_onehot, output)\n",
    "                        houtput = Hmodel(data)\n",
    "                        residual = residual.type(torch.cuda.FloatTensor)\n",
    "                        houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                        ce_loss += Hcriterion(houtput, residual)\n",
    "                        y_pred = F.softmax(output, dim=-1)\n",
    "#                         y_pred = torch.reshape(output,dim=-1)\n",
    "                        bool_R_y_true = torch.eq(torch.ones_like(y_true) - y_true, torch.ones_like(y_true))\n",
    "                        mask_non_y_pred.append(torch.masked_select(y_pred, bool_R_y_true).reshape(-1, nb_digits-1))\n",
    "                        ensemble_probs += y_pred\n",
    "\n",
    "                ensemble_probs = ensemble_probs / (m+1)\n",
    "                ensemble_entropy = torch.sum(-torch.mul(ensemble_probs, torch.log(ensemble_probs + log_offset)), dim=-1).mean()\n",
    "\n",
    "                mask_non_y_pred = torch.stack(mask_non_y_pred, dim=1)\n",
    "\n",
    "#                 print(\"mask_non_y_pred shape is:\", mask_non_y_pred.shape)\n",
    "                assert mask_non_y_pred.shape == (data.size(0), m+1, nb_digits-1)\n",
    "                mask_non_y_pred = mask_non_y_pred / torch.norm(mask_non_y_pred, p=2, dim=-1, keepdim=True)\n",
    "                matrix = torch.matmul(mask_non_y_pred, mask_non_y_pred.permute(0, 2, 1))\n",
    "                log_det = torch.logdet(matrix+det_offset*torch.eye((m+1), device=matrix.device).unsqueeze(0)).mean()\n",
    "\n",
    "                loss = ce_loss - alpha * ensemble_entropy - beta * log_det\n",
    "\n",
    "                losses += loss.item()\n",
    "                ce_losses += ce_loss.item()\n",
    "                ee_losses += ensemble_entropy.item()\n",
    "                det_losses += -log_det.item()\n",
    "\n",
    "                Hoptimizer.zero_grad()\n",
    "                loss.backward(retain_graph = True)\n",
    "                Hoptimizer.step()\n",
    "                if (batch_idx + 1) % 400 == 0:\n",
    "                    print('Train Epoch: Model Number: {} {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                            m+1,i, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                            100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "                if m == 0:\n",
    "                    writer.add_scalar('train/ce_loss_0', ce_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/ee_loss_0', ee_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/det_loss_0', det_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/log_det_0', log_det, i)\n",
    "                if m == 1:\n",
    "                    writer.add_scalar('train/ce_loss_1', ce_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/ee_loss_1', ee_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/det_loss_1', det_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/log_det_1', log_det, i)\n",
    "                if m == 2:\n",
    "                    writer.add_scalar('train/ce_loss_2', ce_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/ee_loss_2', ee_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/det_loss_2', det_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/log_det_2', log_det, i)\n",
    "        models_adp.append(Hmodel)\n",
    "        \n",
    "            # Initialize a random gamma\n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.01)\n",
    "        Gcriterion = nn.MSELoss()\n",
    "        # Start finding the best gamma\n",
    "        for i in range(20):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                    \n",
    "                #Calculate the initial output\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                #Calculate the final output by combining all previous models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp_adp[j]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "                # Covert into one-hot label\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                # Get the currect model output\n",
    "                temp = Hmodel(data)\n",
    "                # Find the current ensemble model output\n",
    "                predicted = output + gamma * temp\n",
    "                # Calculate the loss\n",
    "                loss = Gcriterion(predicted, target_onehot)\n",
    "                loss.backward(retain_graph = True)\n",
    "                # Optimize the gamma\n",
    "                Goptimizer.step()  \n",
    "        gamma_exp_adp[m] = gamma\n",
    "    print(gamma_exp_adp)\n",
    "    writer.close()\n",
    "    return models_adp, gamma_exp_adp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Gradient Boosting Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [12800/60000 (21%)]\tLoss: 0.021916\n",
      "Train Epoch: Model Number: 1 0 [25600/60000 (43%)]\tLoss: 0.023078\n",
      "Train Epoch: Model Number: 1 0 [38400/60000 (64%)]\tLoss: 0.014574\n",
      "Train Epoch: Model Number: 1 0 [51200/60000 (85%)]\tLoss: 0.017474\n",
      "Train Epoch: Model Number: 1 10 [12800/60000 (21%)]\tLoss: 0.023645\n",
      "Train Epoch: Model Number: 1 10 [25600/60000 (43%)]\tLoss: 0.023895\n",
      "Train Epoch: Model Number: 1 10 [38400/60000 (64%)]\tLoss: 0.014529\n",
      "Train Epoch: Model Number: 1 10 [51200/60000 (85%)]\tLoss: 0.022531\n",
      "Train Epoch: Model Number: 1 20 [12800/60000 (21%)]\tLoss: 0.017691\n",
      "Train Epoch: Model Number: 1 20 [25600/60000 (43%)]\tLoss: 0.015025\n",
      "Train Epoch: Model Number: 1 20 [38400/60000 (64%)]\tLoss: 0.014164\n",
      "Train Epoch: Model Number: 1 20 [51200/60000 (85%)]\tLoss: 0.021518\n",
      "Train Epoch: Model Number: 1 30 [12800/60000 (21%)]\tLoss: 0.015692\n",
      "Train Epoch: Model Number: 1 30 [25600/60000 (43%)]\tLoss: 0.014568\n",
      "Train Epoch: Model Number: 1 30 [38400/60000 (64%)]\tLoss: 0.012757\n",
      "Train Epoch: Model Number: 1 30 [51200/60000 (85%)]\tLoss: 0.017339\n",
      "Train Epoch: Model Number: 2 0 [12800/60000 (21%)]\tLoss: 0.022091\n",
      "Train Epoch: Model Number: 2 0 [25600/60000 (43%)]\tLoss: 0.023886\n",
      "Train Epoch: Model Number: 2 0 [38400/60000 (64%)]\tLoss: 0.025036\n",
      "Train Epoch: Model Number: 2 0 [51200/60000 (85%)]\tLoss: 0.026427\n",
      "Train Epoch: Model Number: 2 10 [12800/60000 (21%)]\tLoss: 0.019180\n",
      "Train Epoch: Model Number: 2 10 [25600/60000 (43%)]\tLoss: 0.022250\n",
      "Train Epoch: Model Number: 2 10 [38400/60000 (64%)]\tLoss: 0.026372\n",
      "Train Epoch: Model Number: 2 10 [51200/60000 (85%)]\tLoss: 0.017055\n",
      "Train Epoch: Model Number: 2 20 [12800/60000 (21%)]\tLoss: 0.021377\n",
      "Train Epoch: Model Number: 2 20 [25600/60000 (43%)]\tLoss: 0.018999\n",
      "Train Epoch: Model Number: 2 20 [38400/60000 (64%)]\tLoss: 0.024139\n",
      "Train Epoch: Model Number: 2 20 [51200/60000 (85%)]\tLoss: 0.018362\n",
      "Train Epoch: Model Number: 2 30 [12800/60000 (21%)]\tLoss: 0.020822\n",
      "Train Epoch: Model Number: 2 30 [25600/60000 (43%)]\tLoss: 0.020698\n",
      "Train Epoch: Model Number: 2 30 [38400/60000 (64%)]\tLoss: 0.019255\n",
      "Train Epoch: Model Number: 2 30 [51200/60000 (85%)]\tLoss: 0.021827\n",
      "Train Epoch: Model Number: 3 0 [12800/60000 (21%)]\tLoss: 0.021082\n",
      "Train Epoch: Model Number: 3 0 [25600/60000 (43%)]\tLoss: 0.013956\n",
      "Train Epoch: Model Number: 3 0 [38400/60000 (64%)]\tLoss: 0.024601\n",
      "Train Epoch: Model Number: 3 0 [51200/60000 (85%)]\tLoss: 0.018297\n",
      "Train Epoch: Model Number: 3 10 [12800/60000 (21%)]\tLoss: 0.024589\n",
      "Train Epoch: Model Number: 3 10 [25600/60000 (43%)]\tLoss: 0.020964\n",
      "Train Epoch: Model Number: 3 10 [38400/60000 (64%)]\tLoss: 0.018101\n",
      "Train Epoch: Model Number: 3 10 [51200/60000 (85%)]\tLoss: 0.017764\n",
      "Train Epoch: Model Number: 3 20 [12800/60000 (21%)]\tLoss: 0.019799\n",
      "Train Epoch: Model Number: 3 20 [25600/60000 (43%)]\tLoss: 0.019877\n",
      "Train Epoch: Model Number: 3 20 [38400/60000 (64%)]\tLoss: 0.017849\n",
      "Train Epoch: Model Number: 3 20 [51200/60000 (85%)]\tLoss: 0.017855\n",
      "Train Epoch: Model Number: 3 30 [12800/60000 (21%)]\tLoss: 0.020832\n",
      "Train Epoch: Model Number: 3 30 [25600/60000 (43%)]\tLoss: 0.022674\n",
      "Train Epoch: Model Number: 3 30 [38400/60000 (64%)]\tLoss: 0.021860\n",
      "Train Epoch: Model Number: 3 30 [51200/60000 (85%)]\tLoss: 0.020141\n",
      "tensor([ 0.1789, -0.0662,  0.0360], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "num_of_models = 3\n",
    "models, gamma_exp = GradientBoosting(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: Model Number: 1 0 [51200/60000 (85%)]\tLoss: -4.435918\n",
      "Train Epoch: Model Number: 1 1 [51200/60000 (85%)]\tLoss: -4.439062\n",
      "Train Epoch: Model Number: 1 2 [51200/60000 (85%)]\tLoss: -4.439041\n",
      "Train Epoch: Model Number: 1 3 [51200/60000 (85%)]\tLoss: -4.441585\n",
      "Train Epoch: Model Number: 1 4 [51200/60000 (85%)]\tLoss: -4.442135\n",
      "Train Epoch: Model Number: 1 5 [51200/60000 (85%)]\tLoss: -4.436696\n",
      "Train Epoch: Model Number: 1 6 [51200/60000 (85%)]\tLoss: -4.443707\n",
      "Train Epoch: Model Number: 1 7 [51200/60000 (85%)]\tLoss: -4.437622\n",
      "Train Epoch: Model Number: 1 8 [51200/60000 (85%)]\tLoss: -4.440589\n",
      "Train Epoch: Model Number: 1 9 [51200/60000 (85%)]\tLoss: -4.440548\n",
      "Train Epoch: Model Number: 1 10 [51200/60000 (85%)]\tLoss: -4.440617\n",
      "Train Epoch: Model Number: 1 11 [51200/60000 (85%)]\tLoss: -4.435962\n",
      "Train Epoch: Model Number: 1 12 [51200/60000 (85%)]\tLoss: -4.443596\n",
      "Train Epoch: Model Number: 1 13 [51200/60000 (85%)]\tLoss: -4.443573\n",
      "Train Epoch: Model Number: 1 14 [51200/60000 (85%)]\tLoss: -4.435838\n",
      "Train Epoch: Model Number: 1 15 [51200/60000 (85%)]\tLoss: -4.439165\n",
      "Train Epoch: Model Number: 1 16 [51200/60000 (85%)]\tLoss: -4.437573\n",
      "Train Epoch: Model Number: 1 17 [51200/60000 (85%)]\tLoss: -4.436017\n",
      "Train Epoch: Model Number: 1 18 [51200/60000 (85%)]\tLoss: -4.437772\n",
      "Train Epoch: Model Number: 1 19 [51200/60000 (85%)]\tLoss: -4.436088\n",
      "Train Epoch: Model Number: 1 20 [51200/60000 (85%)]\tLoss: -4.435881\n",
      "Train Epoch: Model Number: 1 21 [51200/60000 (85%)]\tLoss: -4.437483\n",
      "Train Epoch: Model Number: 1 22 [51200/60000 (85%)]\tLoss: -4.429724\n",
      "Train Epoch: Model Number: 1 23 [51200/60000 (85%)]\tLoss: -4.437592\n",
      "Train Epoch: Model Number: 1 24 [51200/60000 (85%)]\tLoss: -4.434388\n",
      "Train Epoch: Model Number: 1 25 [51200/60000 (85%)]\tLoss: -4.445318\n",
      "Train Epoch: Model Number: 1 26 [51200/60000 (85%)]\tLoss: -4.440860\n",
      "Train Epoch: Model Number: 1 27 [51200/60000 (85%)]\tLoss: -4.443654\n",
      "Train Epoch: Model Number: 1 28 [51200/60000 (85%)]\tLoss: -4.441730\n",
      "Train Epoch: Model Number: 1 29 [51200/60000 (85%)]\tLoss: -4.436037\n",
      "Train Epoch: Model Number: 2 0 [51200/60000 (85%)]\tLoss: 1.281278\n",
      "Train Epoch: Model Number: 2 1 [51200/60000 (85%)]\tLoss: 1.275042\n",
      "Train Epoch: Model Number: 2 2 [51200/60000 (85%)]\tLoss: 1.258355\n",
      "Train Epoch: Model Number: 2 3 [51200/60000 (85%)]\tLoss: 1.270223\n",
      "Train Epoch: Model Number: 2 4 [51200/60000 (85%)]\tLoss: 1.307907\n",
      "Train Epoch: Model Number: 2 5 [51200/60000 (85%)]\tLoss: 1.292943\n",
      "Train Epoch: Model Number: 2 6 [51200/60000 (85%)]\tLoss: 1.295150\n",
      "Train Epoch: Model Number: 2 7 [51200/60000 (85%)]\tLoss: 1.213424\n",
      "Train Epoch: Model Number: 2 8 [51200/60000 (85%)]\tLoss: 1.283327\n",
      "Train Epoch: Model Number: 2 9 [51200/60000 (85%)]\tLoss: 1.237090\n",
      "Train Epoch: Model Number: 2 10 [51200/60000 (85%)]\tLoss: 1.276057\n",
      "Train Epoch: Model Number: 2 11 [51200/60000 (85%)]\tLoss: 1.319077\n",
      "Train Epoch: Model Number: 2 12 [51200/60000 (85%)]\tLoss: 1.313898\n",
      "Train Epoch: Model Number: 2 13 [51200/60000 (85%)]\tLoss: 1.219049\n",
      "Train Epoch: Model Number: 2 14 [51200/60000 (85%)]\tLoss: 1.257493\n",
      "Train Epoch: Model Number: 2 15 [51200/60000 (85%)]\tLoss: 1.287788\n",
      "Train Epoch: Model Number: 2 16 [51200/60000 (85%)]\tLoss: 1.293379\n",
      "Train Epoch: Model Number: 2 17 [51200/60000 (85%)]\tLoss: 1.295254\n",
      "Train Epoch: Model Number: 2 18 [51200/60000 (85%)]\tLoss: 1.238796\n",
      "Train Epoch: Model Number: 2 19 [51200/60000 (85%)]\tLoss: 1.296640\n",
      "Train Epoch: Model Number: 2 20 [51200/60000 (85%)]\tLoss: 1.290822\n",
      "Train Epoch: Model Number: 2 21 [51200/60000 (85%)]\tLoss: 1.237148\n",
      "Train Epoch: Model Number: 2 22 [51200/60000 (85%)]\tLoss: 1.291887\n",
      "Train Epoch: Model Number: 2 23 [51200/60000 (85%)]\tLoss: 1.265205\n",
      "Train Epoch: Model Number: 2 24 [51200/60000 (85%)]\tLoss: 1.253670\n",
      "Train Epoch: Model Number: 2 25 [51200/60000 (85%)]\tLoss: 1.253140\n",
      "Train Epoch: Model Number: 2 26 [51200/60000 (85%)]\tLoss: 1.267834\n",
      "Train Epoch: Model Number: 2 27 [51200/60000 (85%)]\tLoss: 1.206324\n",
      "Train Epoch: Model Number: 2 28 [51200/60000 (85%)]\tLoss: 1.316329\n",
      "Train Epoch: Model Number: 2 29 [51200/60000 (85%)]\tLoss: 1.290421\n",
      "Train Epoch: Model Number: 3 0 [51200/60000 (85%)]\tLoss: 6.953382\n",
      "Train Epoch: Model Number: 3 1 [51200/60000 (85%)]\tLoss: 7.005185\n",
      "Train Epoch: Model Number: 3 2 [51200/60000 (85%)]\tLoss: 7.004928\n",
      "Train Epoch: Model Number: 3 3 [51200/60000 (85%)]\tLoss: 6.904191\n",
      "Train Epoch: Model Number: 3 4 [51200/60000 (85%)]\tLoss: 6.979174\n",
      "Train Epoch: Model Number: 3 5 [51200/60000 (85%)]\tLoss: 6.999189\n",
      "Train Epoch: Model Number: 3 6 [51200/60000 (85%)]\tLoss: 6.974176\n",
      "Train Epoch: Model Number: 3 7 [51200/60000 (85%)]\tLoss: 7.070295\n",
      "Train Epoch: Model Number: 3 8 [51200/60000 (85%)]\tLoss: 7.062967\n",
      "Train Epoch: Model Number: 3 9 [51200/60000 (85%)]\tLoss: 6.973169\n",
      "Train Epoch: Model Number: 3 10 [51200/60000 (85%)]\tLoss: 6.999959\n",
      "Train Epoch: Model Number: 3 11 [51200/60000 (85%)]\tLoss: 7.009302\n",
      "Train Epoch: Model Number: 3 12 [51200/60000 (85%)]\tLoss: 6.944064\n",
      "Train Epoch: Model Number: 3 13 [51200/60000 (85%)]\tLoss: 6.971974\n",
      "Train Epoch: Model Number: 3 14 [51200/60000 (85%)]\tLoss: 6.990884\n",
      "Train Epoch: Model Number: 3 15 [51200/60000 (85%)]\tLoss: 7.015228\n",
      "Train Epoch: Model Number: 3 16 [51200/60000 (85%)]\tLoss: 6.975451\n",
      "Train Epoch: Model Number: 3 17 [51200/60000 (85%)]\tLoss: 7.023938\n",
      "Train Epoch: Model Number: 3 18 [51200/60000 (85%)]\tLoss: 6.984907\n",
      "Train Epoch: Model Number: 3 19 [51200/60000 (85%)]\tLoss: 7.068073\n",
      "Train Epoch: Model Number: 3 20 [51200/60000 (85%)]\tLoss: 6.987511\n",
      "Train Epoch: Model Number: 3 21 [51200/60000 (85%)]\tLoss: 6.987630\n",
      "Train Epoch: Model Number: 3 22 [51200/60000 (85%)]\tLoss: 7.050356\n",
      "Train Epoch: Model Number: 3 23 [51200/60000 (85%)]\tLoss: 6.995570\n",
      "Train Epoch: Model Number: 3 24 [51200/60000 (85%)]\tLoss: 6.963621\n",
      "Train Epoch: Model Number: 3 25 [51200/60000 (85%)]\tLoss: 7.034994\n",
      "Train Epoch: Model Number: 3 26 [51200/60000 (85%)]\tLoss: 7.009583\n",
      "Train Epoch: Model Number: 3 27 [51200/60000 (85%)]\tLoss: 7.001712\n",
      "Train Epoch: Model Number: 3 28 [51200/60000 (85%)]\tLoss: 6.993585\n",
      "Train Epoch: Model Number: 3 29 [51200/60000 (85%)]\tLoss: 7.013631\n",
      "tensor([ 0.0122, -0.0125, -0.0010], device='cuda:0', dtype=torch.float64,\n",
      "       grad_fn=<CopySlices>)\n"
     ]
    }
   ],
   "source": [
    "num_of_models = 3\n",
    "models_adp, gamma_exp_adp = ADPGradient(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save or Load submodels and optimized gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the models trained\n",
    "for i in range(num_of_models):\n",
    "    model = models[i]\n",
    "    torch.save(model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(i) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the models trained\n",
    "for i in range(num_of_models):\n",
    "    model = models_adp[i]\n",
    "    torch.save(model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model_adp' + str(i) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized gamma\n",
    "torch.save(gamma_exp, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized gamma\n",
    "torch.save(gamma_exp_adp, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp_adp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NHNet(\n",
      "  (flatten): Flatten()\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "), NHNet(\n",
      "  (flatten): Flatten()\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "), NHNet(\n",
      "  (flatten): Flatten()\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "# Load the models from the local files\n",
    "num_of_models = 3\n",
    "models = []\n",
    "for x in range(num_of_models):\n",
    "    globals()['model%s' % x] = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(1) + '.pth')\n",
    "    models.append(globals()['model%s' % x])\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models from the local files\n",
    "num_of_models = 3\n",
    "models_adp = []\n",
    "for x in range(num_of_models):\n",
    "    globals()['model_adp%s' % x] = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model_adp' + str(1) + '.pth')\n",
    "    models_adp.append(globals()['model_adp%s' % x])\n",
    "print(models_adp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1407,  0.0500, -0.0431], device='cuda:0', dtype=torch.float64,\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the optimized gamma from the local files\n",
    "gamma_exp = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')\n",
    "print(gamma_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized gamma from the local files\n",
    "gamma_exp_adp = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp_adp.txt')\n",
    "print(gamma_exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-941e8f0dbfbb>:10: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 1.5054, Accuracy: 57169/60000 (95.282%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the ensemble model\n",
    "# initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()\n",
    "\n",
    "loss = 0\n",
    "correct = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = initial_model(data)\n",
    "        for i in range(num_of_models):\n",
    "            model = models[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                output = output.cuda()\n",
    "                gamma_temp = gamma_exp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            output = output + gamma_temp * model(data)\n",
    "        loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "loss /= len(train_loader.dataset)  \n",
    "print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "    loss, correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-694060482a16>:10: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 1.5083, Accuracy: 57166/60000 (95.277%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the accuracy of the ensemble model\n",
    "# initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()\n",
    "\n",
    "loss = 0\n",
    "correct = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = initial_model(data)\n",
    "        for i in range(num_of_models):\n",
    "            model = models_adp[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                output = output.cuda()\n",
    "                gamma_temp = gamma_exp_adp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            output = output + gamma_temp * model(data)\n",
    "        loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "loss /= len(train_loader.dataset)  \n",
    "print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "    loss, correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW Attack on Intial Model, Testing Robust Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 42.52 %\n"
     ]
    }
   ],
   "source": [
    "# Attack the initial model using CW attack\n",
    "initial_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW(initial_model, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ensemble CW Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-18-1bd8957a13a4>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self._attack_mode is 'only_original':\n"
     ]
    }
   ],
   "source": [
    "# Create ensemble CW attack\n",
    "class Attack_ensemble(object):\n",
    "    r\"\"\"\n",
    "    Base class for all attacks.\n",
    "    .. note::\n",
    "        It automatically set device to the device where given model is.\n",
    "        It temporarily changes the original model's training mode to `test`\n",
    "        by `.eval()` only during an attack process.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, model, models, gamma):\n",
    "        r\"\"\"\n",
    "        Initializes internal attack state.\n",
    "        Arguments:\n",
    "            name (str) : name of an attack.\n",
    "            model (torch.nn.Module): model to attack.\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack = name\n",
    "        self.model = model\n",
    "        self.models = models\n",
    "        self.gamma = gamma\n",
    "        self.model_name = str(model).split(\"(\")[0]\n",
    "\n",
    "        self.training = model.training\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        self._targeted = 1\n",
    "        self._attack_mode = 'original'\n",
    "        self._return_type = 'float'\n",
    "\n",
    "    def forward(self, *input):\n",
    "        r\"\"\"\n",
    "        It defines the computation performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_attack_mode(self, mode):\n",
    "        r\"\"\"\n",
    "        Set the attack mode.\n",
    "  \n",
    "        Arguments:\n",
    "            mode (str) : 'original' (DEFAULT)\n",
    "                         'targeted' - Use input labels as targeted labels.\n",
    "                         'least_likely' - Use least likely labels as targeted labels.\n",
    "        \"\"\"\n",
    "        if self._attack_mode is 'only_original':\n",
    "            raise ValueError(\"Changing attack mode is not supported in this attack method.\")\n",
    "            \n",
    "        if mode==\"original\":\n",
    "            self._attack_mode = \"original\"\n",
    "            self._targeted = 1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"targeted\":\n",
    "            self._attack_mode = \"targeted\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"least_likely\":\n",
    "            self._attack_mode = \"least_likely\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_least_likely_label\n",
    "        else:\n",
    "            raise ValueError(mode + \" is not a valid mode. [Options : original, targeted, least_likely]\")\n",
    "            \n",
    "    def set_return_type(self, type):\n",
    "        r\"\"\"\n",
    "        Set the return type of adversarial images: `int` or `float`.\n",
    "        Arguments:\n",
    "            type (str) : 'float' or 'int'. (DEFAULT : 'float')\n",
    "        \"\"\"\n",
    "        if type == 'float':\n",
    "            self._return_type = 'float'\n",
    "        elif type == 'int':\n",
    "            self._return_type = 'int'\n",
    "        else:\n",
    "            raise ValueError(type + \" is not a valid type. [Options : float, int]\")\n",
    "\n",
    "    def save(self, save_path, data_loader, verbose=True):\n",
    "        r\"\"\"\n",
    "        Save adversarial images as torch.tensor from given torch.utils.data.DataLoader.\n",
    "        Arguments:\n",
    "            save_path (str) : save_path.\n",
    "            data_loader (torch.utils.data.DataLoader) : data loader.\n",
    "            verbose (bool) : True for displaying detailed information. (DEFAULT : True)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        image_list = []\n",
    "        label_list = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        total_batch = len(data_loader)\n",
    "\n",
    "        for step, (images, labels) in enumerate(data_loader):\n",
    "            adv_images = self.__call__(images, labels)\n",
    "\n",
    "            image_list.append(adv_images.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "\n",
    "            if self._return_type == 'int':\n",
    "                adv_images = adv_images.float()/255\n",
    "\n",
    "            if verbose:\n",
    "                outputs = self.model(adv_images)\n",
    "                for i in range(len(self.models)):\n",
    "                    sub_model = self.models[i]\n",
    "                    outputs = outputs + self.gamma[i] * sub_model(adv_images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.to(self.device)).sum()\n",
    "\n",
    "                acc = 100 * float(correct) / total\n",
    "                print('- Save Progress : %2.2f %% / Accuracy : %2.2f %%' % ((step+1)/total_batch*100, acc), end='\\r')\n",
    "\n",
    "        x = torch.cat(image_list, 0)\n",
    "        y = torch.cat(label_list, 0)\n",
    "        torch.save((x, y), save_path)\n",
    "        print('\\n- Save Complete!')\n",
    "\n",
    "        self._switch_model()\n",
    "        \n",
    "    def _transform_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "        \n",
    "    def _get_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return input labels.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "    \n",
    "    def _get_least_likely_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return least likely labels.\n",
    "        \"\"\"\n",
    "        outputs = self.model(images)\n",
    "        for i in range(len(self.models)):\n",
    "            sub_model = self.models[i]\n",
    "            outputs = outputs + self.gamma[i] * sub_model(images)\n",
    "        _, labels = torch.min(outputs.data, 1)\n",
    "        labels = labels.detach_()\n",
    "        return labels\n",
    "    \n",
    "    def _to_uint(self, images):\n",
    "        r\"\"\"\n",
    "        Function for changing the return type.\n",
    "        Return images as int.\n",
    "        \"\"\"\n",
    "        return (images*255).type(torch.uint8)\n",
    "\n",
    "    def _switch_model(self):\n",
    "        r\"\"\"\n",
    "        Function for changing the training mode of the model.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.model.train()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].eval()\n",
    "\n",
    "    def __str__(self):\n",
    "        info = self.__dict__.copy()\n",
    "        \n",
    "        del_keys = ['model', 'attack']\n",
    "        \n",
    "        for key in info.keys():\n",
    "            if key[0] == \"_\" :\n",
    "                del_keys.append(key)\n",
    "                \n",
    "        for key in del_keys:\n",
    "            del info[key]\n",
    "        \n",
    "        info['attack_mode'] = self._attack_mode\n",
    "        if info['attack_mode'] == 'only_original' :\n",
    "            info['attack_mode'] = 'original'\n",
    "            \n",
    "        info['return_type'] = self._return_type\n",
    "        \n",
    "        return self.attack + \"(\" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + \")\"\n",
    "\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        self.model.eval()\n",
    "        for i in range(len(self.models)):\n",
    "            self.models[i].eval()\n",
    "        images = self.forward(*input, **kwargs)\n",
    "        self._switch_model()\n",
    "\n",
    "        if self._return_type == 'int':\n",
    "            images = self._to_uint(images)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ensemble CW Attack\n",
    "import warnings\n",
    "\n",
    "class CW_Ensemble(Attack_ensemble):\n",
    "    r\"\"\"\n",
    "    CW in the paper 'Towards Evaluating the Robustness of Neural Networks'\n",
    "    [https://arxiv.org/abs/1608.04644]\n",
    "    Distance Measure : L2\n",
    "        \n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        c (float): c in the paper. parameter for box-constraint. (DEFALUT : 1e-4)    \n",
    "            :math:`minimize \\Vert\\frac{1}{2}(tanh(w)+1)-x\\Vert^2_2+c\\cdot f(\\frac{1}{2}(tanh(w)+1))`    \n",
    "        kappa (float): kappa (also written as 'confidence') in the paper. (DEFALUT : 0)\n",
    "            :math:`f(x')=max(max\\{Z(x')_i:i\\neq t\\} -Z(x')_t, - \\kappa)`\n",
    "        steps (int): number of steps. (DEFALUT : 1000)\n",
    "        lr (float): learning rate of the Adam optimizer. (DEFALUT : 0.01)\n",
    "        \n",
    "    .. warning:: With default c, you can't easily get adversarial images. Set higher c like 1.\n",
    "    \n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "          \n",
    "    Examples::\n",
    "        >>> attack = torchattacks.CW(model, targeted=False, c=1e-4, kappa=0, steps=1000, lr=0.01)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "        \n",
    "    .. note:: NOT IMPLEMENTED methods in the paper due to time consuming.\n",
    "    \n",
    "        (1) Binary search for c.\n",
    "        \n",
    "        (2) Choosing best L2 adversaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, models, gamma, c=1e-4, kappa=0, steps=1000, lr=0.01):\n",
    "        super(CW_Ensemble, self).__init__(\"CW\", model, models, gamma)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        labels = self._transform_label(images, labels)\n",
    "\n",
    "        # f-function in the paper\n",
    "        def f(x):\n",
    "            outputs = self.model(x)\n",
    "            for i in range(len(self.models)):\n",
    "                sub_model = self.models[i]\n",
    "                outputs = outputs + self.gamma[i] * sub_model(x)\n",
    "            one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
    "\n",
    "            i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "            j = torch.masked_select(outputs, one_hot_labels.bool())\n",
    "\n",
    "            return torch.clamp(self._targeted*(j-i), min=-self.kappa)\n",
    "\n",
    "        w = torch.zeros_like(images).to(self.device)\n",
    "        w.detach_()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "        prev = 1e10\n",
    "\n",
    "        for step in range(self.steps):\n",
    "\n",
    "            a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "            loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "            loss2 = torch.sum(self.c*f(a))\n",
    "\n",
    "            cost = loss1 + loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Early Stop when loss does not converge.\n",
    "            if step % (self.steps//10) == 0:\n",
    "                if cost > prev:\n",
    "                    warnings.warn(\"Early stopped because the loss is not converged.\")\n",
    "                    return (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "                prev = cost\n",
    "\n",
    "            # print('- CW Attack Progress : %2.2f %%        ' %((step+1)/self.steps*100), end='\\r')\n",
    "\n",
    "        adv_images = (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "\n",
    "        return adv_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CW Attack Ensemble Model, Testing Robust Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 44.53 %\n"
     ]
    }
   ],
   "source": [
    "#Attack the ensemble model\n",
    "initial_model.eval()\n",
    "for i in range(num_of_models):\n",
    "    models[i].eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW_Ensemble(model = initial_model,models = models,gamma = gamma_exp, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        for i in range(num_of_models):\n",
    "            sub_model = models[i]\n",
    "            outputs = outputs + gamma_exp[i] * sub_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robust accuracy: 42.19 %\n"
     ]
    }
   ],
   "source": [
    "#Attack the ensemble model\n",
    "initial_model.eval()\n",
    "for i in range(num_of_models):\n",
    "    models_adp[i].eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW_Ensemble(model = initial_model,models = models_adp,gamma = gamma_exp_adp, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        for i in range(num_of_models):\n",
    "            sub_model = models_adp[i]\n",
    "            outputs = outputs + gamma_exp_adp[i] * sub_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
