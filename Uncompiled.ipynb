{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "import numbers\n",
    "\n",
    "import torchattacks\n",
    "from torchattacks import CW\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#Creating dataset using torch dataloaders\n",
    "batch_size_train = 128\n",
    "batch_size_test = 1000\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))]) # Normalizing dataset\n",
    "\n",
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=transform),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "# Initialize GPU\n",
    "use_cuda = True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial model structure\n",
    "class HNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(HNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1) # add this because I need one-hot label and MSE loss\n",
    "        return x     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submodel Structure for training residual\n",
    "class NHNet(nn.Module):    \n",
    "    def __init__(self):\n",
    "        super(NHNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim = 1) # add this because I need one-hot label and MSE loss\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize some data structures to store useful data\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the initial model\n",
    "initial_model = HNet()\n",
    "\n",
    "#Create the optimizer for the initial model\n",
    "optimizer = optim.Adam(initial_model.parameters(), lr=0.003)\n",
    "\n",
    "# Create Loss function for the intial model\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "#Change model into cuda mode\n",
    "if torch.cuda.is_available():\n",
    "    initial_model = initial_model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Model Training Function\n",
    "def train(epoch):\n",
    "    initial_model.train()\n",
    "#     exp_lr_scheduler.step()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = initial_model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "        train_counter.append(\n",
    "                (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "        # torch.save(initial_model.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/model.pth')\n",
    "        # torch.save(optimizer.state_dict(), 'C:/Users/cozyn/Desktop/Research/Adversarial-Machine-Learning/results/optimizer.pth')\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial Model Evaluating Function\n",
    "def evaluate(data_loader):\n",
    "    initial_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = Variable(data, volatile=True), Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                data = data.cuda()\n",
    "                target = target.cuda()\n",
    "        \n",
    "            output = initial_model(data)\n",
    "        \n",
    "            loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    test_losses.append(loss)    \n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        loss, correct, len(data_loader.dataset),\n",
    "        100. * correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "n_epochs = 65\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save this model so that I dont have to train again in the future\n",
    "torch.save(initial_model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "HNet(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Load the model from local file\n",
    "initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to calculate mse residual based on the wiki\n",
    "def mseresidual(y, F):\n",
    "    residual = y - F\n",
    "    absolute = torch.abs(residual)\n",
    "    residual = residual / torch.max(absolute)\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Function used to train and find optimal gamma for submodels\n",
    "#input: intial model that's already trained\n",
    "#M is number of submodels needed to be trained\n",
    "def GradientBoosting(initial_model, M):\n",
    "    gamma_exp = torch.ones([M], dtype = torch.float64) # used to hold the final optimized gamma\n",
    "    models = [] # used to hold all the models\n",
    "    residual_list = [] # used to hold the residual of each batch calculated\n",
    "    writer = SummaryWriter()\n",
    "    for m in range(M):\n",
    "        # Intialize submodels\n",
    "        Hmodel = NHNet()\n",
    "        Hcriterion = nn.MSELoss()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp = gamma_exp.cuda()\n",
    "            Hcriterion = Hcriterion.cuda()\n",
    "            \n",
    "        # Start Training\n",
    "        epoch = 60\n",
    "        Hoptimizer = optim.Adam(Hmodel.parameters(), lr=0.001)\n",
    "        for i in range(epoch):\n",
    "            Hmodel.train()\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                # Create one-hot label target tensor\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                # Calculate F(x)\n",
    "                output = initial_model(data)\n",
    "                # Calculate the output from all the models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        output = output.cuda()\n",
    "                        model = model.cuda()\n",
    "                    output = output + gamma_exp[j] * model(data)\n",
    "#                 print(\"output is:\", output)\n",
    "                #Convert into Onehot label so that it would be able to calculate the residual\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                #Calculate Residual\n",
    "#                 print(\"target_onehot is:\", target_onehot)\n",
    "#                 print(\"output is:\", output)\n",
    "                residual = mseresidual(target_onehot, output)\n",
    "                houtput = Hmodel(data)\n",
    "#                 print(\"houtput is:\", houtput)\n",
    "                residual = residual.type(torch.cuda.FloatTensor)\n",
    "                houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                #Calculate the loss\n",
    "                loss = Hcriterion(houtput, residual)\n",
    "                Hoptimizer.zero_grad()\n",
    "                loss.backward(retain_graph = True)\n",
    "                Hoptimizer.step()\n",
    "                # Print out current Process\n",
    "#                 if (batch_idx + 1)% 100 == 0 and i % 10 == 0:\n",
    "                if (batch_idx + 1) % 400 == 0:\n",
    "                    print('Train Epoch: Model Number: {} {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                        m+1,i, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                        100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "        models.append(Hmodel)\n",
    "        \n",
    "        # Initialize a random gamma\n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.01)\n",
    "        Gcriterion = nn.MSELoss()\n",
    "        # Start finding the best gamma\n",
    "        for i in range(20):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                    \n",
    "                #Calculate the initial output\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                #Calculate the final output by combining all previous models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp[j]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "                # Covert into one-hot label\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                # Get the currect model output\n",
    "                temp = Hmodel(data)\n",
    "                # Find the current ensemble model output\n",
    "                predicted = output + gamma * temp\n",
    "                # Calculate the loss\n",
    "                loss = Gcriterion(predicted, target_onehot)\n",
    "                loss.backward(retain_graph = True)\n",
    "                # Optimize the gamma\n",
    "                Goptimizer.step()  \n",
    "        gamma_exp[m] = gamma\n",
    "    print(gamma_exp)\n",
    "    return models, gamma_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_offset = 1e-20\n",
    "det_offset = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADPGradient(initial_model, M):\n",
    "    gamma_exp = torch.ones([M], dtype = torch.float64)\n",
    "    models = [] # used to hold all the models\n",
    "    residual_list = [] # used to hold the residual of each batch calculated\n",
    "    epoch = 40\n",
    "    writer = SummaryWriter()\n",
    "    for m in range(M):\n",
    "        # Intialize submodels\n",
    "        Hmodel = NHNet()\n",
    "        Hcriterion = nn.MSELoss()\n",
    "        if torch.cuda.is_available():\n",
    "            Hmodel = Hmodel.cuda()\n",
    "            gamma_exp = gamma_exp.cuda()\n",
    "            Hcriterion = Hcriterion.cuda()\n",
    "            params = []\n",
    "            params += list(Hmodel.parameters())\n",
    "            for j in models:\n",
    "                params += list(j.parameters())\n",
    "            Hoptimizer = optim.Adam(params, lr=0.001, weight_decay=1e-4, eps=1e-7)\n",
    "        for i in range(epoch):\n",
    "            Hmodel.train()\n",
    "            for j in models:\n",
    "                j.train()\n",
    "                \n",
    "            losses = 0\n",
    "            ce_losses = 0\n",
    "            ee_losses = 0\n",
    "            det_losses = 0\n",
    "            alpha = 2.0\n",
    "            beta = 0.5\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                y_true = torch.zeros(data.size(0), nb_digits).cuda()\n",
    "                y_true.scatter_(1, target.view(-1,1), 1)\n",
    "                ce_loss = 0\n",
    "                mask_non_y_pred = []\n",
    "                ensemble_probs = 0\n",
    "                for k in range(m+1):\n",
    "                    output = initial_model(data)\n",
    "                    target = target.view(-1,1)\n",
    "                    target_onehot.zero_()\n",
    "                    target_onehot.scatter_(1, target, 1)\n",
    "                    for j in range(m):\n",
    "                        model = models[j]\n",
    "                        if torch.cuda.is_available():\n",
    "                            model = model.cuda()\n",
    "                            output = output.cuda()\n",
    "                            gamma_temp = gamma_exp[j]\n",
    "                            gamma_temp = gamma_temp.cuda()\n",
    "                        if k < m and j == k:\n",
    "                            residual = mseresidual(target_onehot, output)\n",
    "                            houtput = model(data)\n",
    "                            residual = residual.type(torch.cuda.FloatTensor)\n",
    "                            houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                            ce_loss += Hcriterion(houtput, residual)\n",
    "                            y_pred = F.softmax(output, dim=-1)\n",
    "#                             y_pred = torch.reshape(output,dim=-1)\n",
    "                            bool_R_y_true = torch.eq(torch.ones_like(y_true) - y_true, torch.ones_like(y_true))\n",
    "                            mask_non_y_pred.append(torch.masked_select(y_pred, bool_R_y_true).reshape(-1, nb_digits-1))\n",
    "                            ensemble_probs += y_pred\n",
    "                        output = output + gamma_temp * model(data)\n",
    "                    \n",
    "                    if k == m:\n",
    "                        residual = mseresidual(target_onehot, output)\n",
    "                        houtput = Hmodel(data)\n",
    "                        residual = residual.type(torch.cuda.FloatTensor)\n",
    "                        houtput = houtput.type(torch.cuda.FloatTensor)\n",
    "                        ce_loss += Hcriterion(houtput, residual)\n",
    "                        y_pred = F.softmax(output, dim=-1)\n",
    "#                         y_pred = torch.reshape(output,dim=-1)\n",
    "                        bool_R_y_true = torch.eq(torch.ones_like(y_true) - y_true, torch.ones_like(y_true))\n",
    "                        mask_non_y_pred.append(torch.masked_select(y_pred, bool_R_y_true).reshape(-1, nb_digits-1))\n",
    "                        ensemble_probs += y_pred\n",
    "\n",
    "                ensemble_probs = ensemble_probs / (m+1)\n",
    "                ensemble_entropy = torch.sum(-torch.mul(ensemble_probs, torch.log(ensemble_probs + log_offset)), dim=-1).mean()\n",
    "\n",
    "                mask_non_y_pred = torch.stack(mask_non_y_pred, dim=1)\n",
    "\n",
    "#                 print(\"mask_non_y_pred shape is:\", mask_non_y_pred.shape)\n",
    "                assert mask_non_y_pred.shape == (data.size(0), m+1, nb_digits-1)\n",
    "                mask_non_y_pred = mask_non_y_pred / torch.norm(mask_non_y_pred, p=2, dim=-1, keepdim=True)\n",
    "                matrix = torch.matmul(mask_non_y_pred, mask_non_y_pred.permute(0, 2, 1))\n",
    "                log_det = torch.logdet(matrix+det_offset*torch.eye((m+1), device=matrix.device).unsqueeze(0)).mean()\n",
    "\n",
    "                loss = ce_loss - alpha * ensemble_entropy - beta * log_det\n",
    "\n",
    "                losses += loss.item()\n",
    "                ce_losses += ce_loss.item()\n",
    "                ee_losses += ensemble_entropy.item()\n",
    "                det_losses += -log_det.item()\n",
    "\n",
    "                Hoptimizer.zero_grad()\n",
    "                loss.backward(retain_graph = True)\n",
    "                Hoptimizer.step()\n",
    "                if (batch_idx + 1) % 400 == 0:\n",
    "                    print('Train Epoch: Model Number: {} {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                            m+1,i, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                            100. * (batch_idx + 1) / len(train_loader), loss.item()))\n",
    "                if m == 0:\n",
    "                    writer.add_scalar('train/ce_loss_0', ce_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/ee_loss_0', ee_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/det_loss_0', det_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/log_det_0', log_det, i)\n",
    "                if m == 1:\n",
    "                    writer.add_scalar('train/ce_loss_1', ce_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/ee_loss_1', ee_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/det_loss_1', det_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/log_det_1', log_det, i)\n",
    "                if m == 2:\n",
    "                    writer.add_scalar('train/ce_loss_2', ce_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/ee_loss_2', ee_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/det_loss_2', det_losses/len(train_loader), i)\n",
    "                    writer.add_scalar('train/log_det_2', log_det, i)\n",
    "        models.append(Hmodel)\n",
    "        \n",
    "            # Initialize a random gamma\n",
    "        gamma = torch.rand(1, requires_grad=True, device=\"cuda\")\n",
    "        Goptimizer = optim.Adam([gamma], lr=0.01)\n",
    "        Gcriterion = nn.MSELoss()\n",
    "        # Start finding the best gamma\n",
    "        for i in range(20):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = Variable(data), Variable(target)\n",
    "                nb_digits = 10\n",
    "                target_onehot = torch.FloatTensor(data.shape[0], nb_digits)\n",
    "                if torch.cuda.is_available():\n",
    "                    data = data.cuda()\n",
    "                    target = target.cuda()\n",
    "                    target_onehot = target_onehot.cuda()\n",
    "                    Hmodel = Hmodel.cuda()\n",
    "                    gamma = gamma.cuda()\n",
    "                    \n",
    "                #Calculate the initial output\n",
    "                Goptimizer.zero_grad()  \n",
    "                output = initial_model(data)\n",
    "                #Calculate the final output by combining all previous models\n",
    "                for j in range(m):\n",
    "                    model = models[j]\n",
    "                    if torch.cuda.is_available():\n",
    "                        model = model.cuda()\n",
    "                        output = output.cuda()\n",
    "                        gamma_temp = gamma_exp[j]\n",
    "                        gamma_temp = gamma_temp.cuda()\n",
    "                    output = output + gamma_temp * model(data)\n",
    "                # Covert into one-hot label\n",
    "                target = target.view(-1,1)\n",
    "                target_onehot.zero_()\n",
    "                target_onehot.scatter_(1, target, 1)\n",
    "                # Get the currect model output\n",
    "                temp = Hmodel(data)\n",
    "                # Find the current ensemble model output\n",
    "                predicted = output + gamma * temp\n",
    "                # Calculate the loss\n",
    "                loss = Gcriterion(predicted, target_onehot)\n",
    "                loss.backward(retain_graph = True)\n",
    "                # Optimize the gamma\n",
    "                Goptimizer.step()  \n",
    "        gamma_exp[m] = gamma\n",
    "    print(gamma_exp)\n",
    "    writer.close()\n",
    "    return models, gamma_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5\n",
      "Train Epoch: Model Number: 1 0 [10112/60000 (17%)]\tLoss: 0.026959\n",
      "Train Epoch: Model Number: 1 0 [10240/60000 (17%)]\tLoss: 0.022511\n",
      "Train Epoch: Model Number: 1 0 [10368/60000 (17%)]\tLoss: 0.019294\n",
      "Train Epoch: Model Number: 1 0 [10496/60000 (17%)]\tLoss: 0.022438\n",
      "Train Epoch: Model Number: 1 0 [10624/60000 (18%)]\tLoss: 0.019337\n",
      "Train Epoch: Model Number: 1 0 [10752/60000 (18%)]\tLoss: 0.017757\n",
      "Train Epoch: Model Number: 1 0 [10880/60000 (18%)]\tLoss: 0.025436\n",
      "Train Epoch: Model Number: 1 0 [11008/60000 (18%)]\tLoss: 0.022444\n",
      "Train Epoch: Model Number: 1 0 [11136/60000 (19%)]\tLoss: 0.014707\n",
      "Train Epoch: Model Number: 1 0 [11264/60000 (19%)]\tLoss: 0.019333\n",
      "Train Epoch: Model Number: 1 0 [11392/60000 (19%)]\tLoss: 0.014638\n",
      "Train Epoch: Model Number: 1 0 [11520/60000 (19%)]\tLoss: 0.024074\n",
      "Train Epoch: Model Number: 1 0 [11648/60000 (19%)]\tLoss: 0.019305\n",
      "Train Epoch: Model Number: 1 0 [11776/60000 (20%)]\tLoss: 0.014634\n",
      "Train Epoch: Model Number: 1 0 [11904/60000 (20%)]\tLoss: 0.020897\n",
      "Train Epoch: Model Number: 1 0 [12032/60000 (20%)]\tLoss: 0.017792\n",
      "Train Epoch: Model Number: 1 0 [12160/60000 (20%)]\tLoss: 0.017600\n",
      "Train Epoch: Model Number: 1 0 [12288/60000 (20%)]\tLoss: 0.020903\n",
      "Train Epoch: Model Number: 1 0 [12416/60000 (21%)]\tLoss: 0.019347\n",
      "Train Epoch: Model Number: 1 0 [12544/60000 (21%)]\tLoss: 0.017806\n",
      "Train Epoch: Model Number: 1 0 [12672/60000 (21%)]\tLoss: 0.025402\n",
      "Train Epoch: Model Number: 1 0 [12800/60000 (21%)]\tLoss: 0.019499\n",
      "Train Epoch: Model Number: 1 0 [12928/60000 (22%)]\tLoss: 0.016160\n",
      "Train Epoch: Model Number: 1 0 [13056/60000 (22%)]\tLoss: 0.020858\n",
      "Train Epoch: Model Number: 1 0 [13184/60000 (22%)]\tLoss: 0.020813\n",
      "Train Epoch: Model Number: 1 0 [13312/60000 (22%)]\tLoss: 0.022358\n",
      "Train Epoch: Model Number: 1 0 [13440/60000 (22%)]\tLoss: 0.018300\n",
      "Train Epoch: Model Number: 1 0 [13568/60000 (23%)]\tLoss: 0.020850\n",
      "Train Epoch: Model Number: 1 0 [13696/60000 (23%)]\tLoss: 0.016195\n",
      "Train Epoch: Model Number: 1 0 [13824/60000 (23%)]\tLoss: 0.020919\n",
      "Train Epoch: Model Number: 1 0 [13952/60000 (23%)]\tLoss: 0.020842\n",
      "Train Epoch: Model Number: 1 0 [14080/60000 (23%)]\tLoss: 0.013185\n",
      "Train Epoch: Model Number: 1 0 [14208/60000 (24%)]\tLoss: 0.022456\n",
      "Train Epoch: Model Number: 1 0 [14336/60000 (24%)]\tLoss: 0.019422\n",
      "Train Epoch: Model Number: 1 0 [14464/60000 (24%)]\tLoss: 0.020374\n",
      "Train Epoch: Model Number: 1 0 [14592/60000 (24%)]\tLoss: 0.016332\n",
      "Train Epoch: Model Number: 1 0 [14720/60000 (25%)]\tLoss: 0.016240\n",
      "Train Epoch: Model Number: 1 0 [14848/60000 (25%)]\tLoss: 0.013140\n",
      "Train Epoch: Model Number: 1 0 [14976/60000 (25%)]\tLoss: 0.025570\n",
      "Train Epoch: Model Number: 1 0 [15104/60000 (25%)]\tLoss: 0.011589\n",
      "Train Epoch: Model Number: 1 0 [15232/60000 (25%)]\tLoss: 0.019330\n",
      "Train Epoch: Model Number: 1 0 [15360/60000 (26%)]\tLoss: 0.011602\n",
      "Train Epoch: Model Number: 1 0 [15488/60000 (26%)]\tLoss: 0.017817\n",
      "Train Epoch: Model Number: 1 0 [15616/60000 (26%)]\tLoss: 0.025589\n",
      "Train Epoch: Model Number: 1 0 [15744/60000 (26%)]\tLoss: 0.021384\n",
      "Train Epoch: Model Number: 1 0 [15872/60000 (26%)]\tLoss: 0.022520\n",
      "Train Epoch: Model Number: 1 0 [16000/60000 (27%)]\tLoss: 0.017772\n",
      "Train Epoch: Model Number: 1 0 [16128/60000 (27%)]\tLoss: 0.017778\n",
      "Train Epoch: Model Number: 1 0 [16256/60000 (27%)]\tLoss: 0.014513\n",
      "Train Epoch: Model Number: 1 0 [16384/60000 (27%)]\tLoss: 0.017383\n",
      "Train Epoch: Model Number: 1 0 [16512/60000 (28%)]\tLoss: 0.019426\n",
      "Train Epoch: Model Number: 1 0 [16640/60000 (28%)]\tLoss: 0.014656\n",
      "Train Epoch: Model Number: 1 0 [16768/60000 (28%)]\tLoss: 0.019268\n",
      "Train Epoch: Model Number: 1 0 [16896/60000 (28%)]\tLoss: 0.021923\n",
      "Train Epoch: Model Number: 1 0 [17024/60000 (28%)]\tLoss: 0.028633\n",
      "Train Epoch: Model Number: 1 0 [17152/60000 (29%)]\tLoss: 0.016238\n",
      "Train Epoch: Model Number: 1 0 [17280/60000 (29%)]\tLoss: 0.018539\n",
      "Train Epoch: Model Number: 1 0 [17408/60000 (29%)]\tLoss: 0.019137\n",
      "Train Epoch: Model Number: 1 0 [17536/60000 (29%)]\tLoss: 0.022354\n",
      "Train Epoch: Model Number: 1 0 [17664/60000 (29%)]\tLoss: 0.016255\n",
      "Train Epoch: Model Number: 1 0 [17792/60000 (30%)]\tLoss: 0.023817\n",
      "Train Epoch: Model Number: 1 0 [17920/60000 (30%)]\tLoss: 0.017906\n",
      "Train Epoch: Model Number: 1 0 [18048/60000 (30%)]\tLoss: 0.020765\n",
      "Train Epoch: Model Number: 1 0 [18176/60000 (30%)]\tLoss: 0.016276\n",
      "Train Epoch: Model Number: 1 0 [18304/60000 (30%)]\tLoss: 0.025491\n",
      "Train Epoch: Model Number: 1 0 [18432/60000 (31%)]\tLoss: 0.022441\n",
      "Train Epoch: Model Number: 1 0 [18560/60000 (31%)]\tLoss: 0.030614\n",
      "Train Epoch: Model Number: 1 0 [18688/60000 (31%)]\tLoss: 0.022889\n",
      "Train Epoch: Model Number: 1 0 [18816/60000 (31%)]\tLoss: 0.012443\n",
      "Train Epoch: Model Number: 1 0 [18944/60000 (32%)]\tLoss: 0.016255\n",
      "Train Epoch: Model Number: 1 0 [19072/60000 (32%)]\tLoss: 0.019289\n",
      "Train Epoch: Model Number: 1 0 [19200/60000 (32%)]\tLoss: 0.020736\n",
      "Train Epoch: Model Number: 1 0 [19328/60000 (32%)]\tLoss: 0.026936\n",
      "Train Epoch: Model Number: 1 0 [19456/60000 (32%)]\tLoss: 0.019210\n",
      "Train Epoch: Model Number: 1 0 [19584/60000 (33%)]\tLoss: 0.019361\n",
      "Train Epoch: Model Number: 1 0 [19712/60000 (33%)]\tLoss: 0.014615\n",
      "Train Epoch: Model Number: 1 0 [19840/60000 (33%)]\tLoss: 0.016257\n",
      "Train Epoch: Model Number: 1 0 [19968/60000 (33%)]\tLoss: 0.013078\n",
      "Train Epoch: Model Number: 1 0 [20096/60000 (33%)]\tLoss: 0.017842\n",
      "Train Epoch: Model Number: 1 0 [20224/60000 (34%)]\tLoss: 0.016296\n",
      "Train Epoch: Model Number: 1 0 [20352/60000 (34%)]\tLoss: 0.019161\n",
      "Train Epoch: Model Number: 1 0 [20480/60000 (34%)]\tLoss: 0.020780\n",
      "Train Epoch: Model Number: 1 0 [20608/60000 (34%)]\tLoss: 0.013119\n",
      "Train Epoch: Model Number: 1 0 [20736/60000 (35%)]\tLoss: 0.020745\n",
      "Train Epoch: Model Number: 1 0 [20864/60000 (35%)]\tLoss: 0.022394\n",
      "Train Epoch: Model Number: 1 0 [20992/60000 (35%)]\tLoss: 0.019385\n",
      "Train Epoch: Model Number: 1 0 [21120/60000 (35%)]\tLoss: 0.016220\n",
      "Train Epoch: Model Number: 1 0 [21248/60000 (35%)]\tLoss: 0.017779\n",
      "Train Epoch: Model Number: 1 0 [21376/60000 (36%)]\tLoss: 0.024106\n",
      "Train Epoch: Model Number: 1 0 [21504/60000 (36%)]\tLoss: 0.022180\n",
      "Train Epoch: Model Number: 1 0 [21632/60000 (36%)]\tLoss: 0.019253\n",
      "Train Epoch: Model Number: 1 0 [21760/60000 (36%)]\tLoss: 0.016157\n",
      "Train Epoch: Model Number: 1 0 [21888/60000 (36%)]\tLoss: 0.014764\n",
      "Train Epoch: Model Number: 1 0 [22016/60000 (37%)]\tLoss: 0.017800\n",
      "Train Epoch: Model Number: 1 0 [22144/60000 (37%)]\tLoss: 0.022485\n",
      "Train Epoch: Model Number: 1 0 [22272/60000 (37%)]\tLoss: 0.019274\n",
      "Train Epoch: Model Number: 1 0 [22400/60000 (37%)]\tLoss: 0.026834\n",
      "Train Epoch: Model Number: 1 0 [22528/60000 (38%)]\tLoss: 0.017875\n",
      "Train Epoch: Model Number: 1 0 [22656/60000 (38%)]\tLoss: 0.016277\n",
      "Train Epoch: Model Number: 1 0 [22784/60000 (38%)]\tLoss: 0.012864\n",
      "Train Epoch: Model Number: 1 0 [22912/60000 (38%)]\tLoss: 0.017897\n",
      "Train Epoch: Model Number: 1 0 [23040/60000 (38%)]\tLoss: 0.020738\n",
      "Train Epoch: Model Number: 1 0 [23168/60000 (39%)]\tLoss: 0.024123\n",
      "Train Epoch: Model Number: 1 0 [23296/60000 (39%)]\tLoss: 0.022358\n",
      "Train Epoch: Model Number: 1 0 [23424/60000 (39%)]\tLoss: 0.013208\n",
      "Train Epoch: Model Number: 1 0 [23552/60000 (39%)]\tLoss: 0.020885\n",
      "Train Epoch: Model Number: 1 0 [23680/60000 (39%)]\tLoss: 0.019387\n",
      "Train Epoch: Model Number: 1 0 [23808/60000 (40%)]\tLoss: 0.014713\n",
      "Train Epoch: Model Number: 1 0 [23936/60000 (40%)]\tLoss: 0.019311\n",
      "Train Epoch: Model Number: 1 0 [24064/60000 (40%)]\tLoss: 0.016173\n",
      "Train Epoch: Model Number: 1 0 [24192/60000 (40%)]\tLoss: 0.020939\n",
      "Train Epoch: Model Number: 1 0 [24320/60000 (41%)]\tLoss: 0.013100\n",
      "Train Epoch: Model Number: 1 0 [24448/60000 (41%)]\tLoss: 0.022406\n",
      "Train Epoch: Model Number: 1 0 [24576/60000 (41%)]\tLoss: 0.019392\n",
      "Train Epoch: Model Number: 1 0 [24704/60000 (41%)]\tLoss: 0.022479\n",
      "Train Epoch: Model Number: 1 0 [24832/60000 (41%)]\tLoss: 0.019353\n",
      "Train Epoch: Model Number: 1 0 [24960/60000 (42%)]\tLoss: 0.014719\n",
      "Train Epoch: Model Number: 1 0 [25088/60000 (42%)]\tLoss: 0.016273\n",
      "Train Epoch: Model Number: 1 0 [25216/60000 (42%)]\tLoss: 0.013105\n",
      "Train Epoch: Model Number: 1 0 [25344/60000 (42%)]\tLoss: 0.022627\n",
      "Train Epoch: Model Number: 1 0 [25472/60000 (42%)]\tLoss: 0.022427\n",
      "Train Epoch: Model Number: 1 0 [25600/60000 (43%)]\tLoss: 0.011563\n",
      "Train Epoch: Model Number: 1 0 [25728/60000 (43%)]\tLoss: 0.022264\n",
      "Train Epoch: Model Number: 1 0 [25856/60000 (43%)]\tLoss: 0.016320\n",
      "Train Epoch: Model Number: 1 0 [25984/60000 (43%)]\tLoss: 0.024093\n",
      "Train Epoch: Model Number: 1 0 [26112/60000 (43%)]\tLoss: 0.027906\n",
      "Train Epoch: Model Number: 1 0 [26240/60000 (44%)]\tLoss: 0.017921\n",
      "Train Epoch: Model Number: 1 0 [26368/60000 (44%)]\tLoss: 0.020799\n",
      "Train Epoch: Model Number: 1 0 [26496/60000 (44%)]\tLoss: 0.020839\n",
      "Train Epoch: Model Number: 1 0 [26624/60000 (44%)]\tLoss: 0.019982\n",
      "Train Epoch: Model Number: 1 0 [26752/60000 (45%)]\tLoss: 0.014701\n",
      "Train Epoch: Model Number: 1 0 [26880/60000 (45%)]\tLoss: 0.013166\n",
      "Train Epoch: Model Number: 1 0 [27008/60000 (45%)]\tLoss: 0.015610\n",
      "Train Epoch: Model Number: 1 0 [27136/60000 (45%)]\tLoss: 0.022291\n",
      "Train Epoch: Model Number: 1 0 [27264/60000 (45%)]\tLoss: 0.018631\n",
      "Train Epoch: Model Number: 1 0 [27392/60000 (46%)]\tLoss: 0.014602\n",
      "Train Epoch: Model Number: 1 0 [27520/60000 (46%)]\tLoss: 0.020700\n",
      "Train Epoch: Model Number: 1 0 [27648/60000 (46%)]\tLoss: 0.016327\n",
      "Train Epoch: Model Number: 1 0 [27776/60000 (46%)]\tLoss: 0.019251\n",
      "Train Epoch: Model Number: 1 0 [27904/60000 (46%)]\tLoss: 0.016273\n",
      "Train Epoch: Model Number: 1 0 [28032/60000 (47%)]\tLoss: 0.022421\n",
      "Train Epoch: Model Number: 1 0 [28160/60000 (47%)]\tLoss: 0.013189\n",
      "Train Epoch: Model Number: 1 0 [28288/60000 (47%)]\tLoss: 0.020875\n",
      "Train Epoch: Model Number: 1 0 [28416/60000 (47%)]\tLoss: 0.020582\n",
      "Train Epoch: Model Number: 1 0 [28544/60000 (48%)]\tLoss: 0.023418\n",
      "Train Epoch: Model Number: 1 0 [28672/60000 (48%)]\tLoss: 0.016272\n",
      "Train Epoch: Model Number: 1 0 [28800/60000 (48%)]\tLoss: 0.022466\n",
      "Train Epoch: Model Number: 1 0 [28928/60000 (48%)]\tLoss: 0.011493\n",
      "Train Epoch: Model Number: 1 0 [29056/60000 (48%)]\tLoss: 0.019368\n",
      "Train Epoch: Model Number: 1 0 [29184/60000 (49%)]\tLoss: 0.020606\n",
      "Train Epoch: Model Number: 1 0 [29312/60000 (49%)]\tLoss: 0.020997\n",
      "Train Epoch: Model Number: 1 0 [29440/60000 (49%)]\tLoss: 0.026897\n",
      "Train Epoch: Model Number: 1 0 [29568/60000 (49%)]\tLoss: 0.019255\n",
      "Train Epoch: Model Number: 1 0 [29696/60000 (49%)]\tLoss: 0.022423\n",
      "Train Epoch: Model Number: 1 0 [29824/60000 (50%)]\tLoss: 0.014806\n",
      "Train Epoch: Model Number: 1 0 [29952/60000 (50%)]\tLoss: 0.025397\n",
      "Train Epoch: Model Number: 1 0 [30080/60000 (50%)]\tLoss: 0.022206\n",
      "Train Epoch: Model Number: 1 0 [30208/60000 (50%)]\tLoss: 0.019184\n",
      "Train Epoch: Model Number: 1 0 [30336/60000 (51%)]\tLoss: 0.014692\n",
      "Train Epoch: Model Number: 1 0 [30464/60000 (51%)]\tLoss: 0.017585\n",
      "Train Epoch: Model Number: 1 0 [30592/60000 (51%)]\tLoss: 0.019387\n",
      "Train Epoch: Model Number: 1 0 [30720/60000 (51%)]\tLoss: 0.020964\n",
      "Train Epoch: Model Number: 1 0 [30848/60000 (51%)]\tLoss: 0.017895\n",
      "Train Epoch: Model Number: 1 0 [30976/60000 (52%)]\tLoss: 0.023830\n",
      "Train Epoch: Model Number: 1 0 [31104/60000 (52%)]\tLoss: 0.022303\n",
      "Train Epoch: Model Number: 1 0 [31232/60000 (52%)]\tLoss: 0.026545\n",
      "Train Epoch: Model Number: 1 0 [31360/60000 (52%)]\tLoss: 0.017442\n",
      "Train Epoch: Model Number: 1 0 [31488/60000 (52%)]\tLoss: 0.020888\n",
      "Train Epoch: Model Number: 1 0 [31616/60000 (53%)]\tLoss: 0.020436\n",
      "Train Epoch: Model Number: 1 0 [31744/60000 (53%)]\tLoss: 0.023825\n",
      "Train Epoch: Model Number: 1 0 [31872/60000 (53%)]\tLoss: 0.023061\n",
      "Train Epoch: Model Number: 1 0 [32000/60000 (53%)]\tLoss: 0.017812\n",
      "Train Epoch: Model Number: 1 0 [32128/60000 (54%)]\tLoss: 0.017926\n",
      "Train Epoch: Model Number: 1 0 [32256/60000 (54%)]\tLoss: 0.020772\n",
      "Train Epoch: Model Number: 1 0 [32384/60000 (54%)]\tLoss: 0.017609\n",
      "Train Epoch: Model Number: 1 0 [32512/60000 (54%)]\tLoss: 0.014749\n",
      "Train Epoch: Model Number: 1 0 [32640/60000 (54%)]\tLoss: 0.023526\n",
      "Train Epoch: Model Number: 1 0 [32768/60000 (55%)]\tLoss: 0.017731\n",
      "Train Epoch: Model Number: 1 0 [32896/60000 (55%)]\tLoss: 0.017659\n",
      "Train Epoch: Model Number: 1 0 [33024/60000 (55%)]\tLoss: 0.014760\n",
      "Train Epoch: Model Number: 1 0 [33152/60000 (55%)]\tLoss: 0.011622\n",
      "Train Epoch: Model Number: 1 0 [33280/60000 (55%)]\tLoss: 0.019335\n",
      "Train Epoch: Model Number: 1 0 [33408/60000 (56%)]\tLoss: 0.013294\n",
      "Train Epoch: Model Number: 1 0 [33536/60000 (56%)]\tLoss: 0.022445\n",
      "Train Epoch: Model Number: 1 0 [33664/60000 (56%)]\tLoss: 0.016232\n",
      "Train Epoch: Model Number: 1 0 [33792/60000 (56%)]\tLoss: 0.022407\n",
      "Train Epoch: Model Number: 1 0 [33920/60000 (57%)]\tLoss: 0.022370\n",
      "Train Epoch: Model Number: 1 0 [34048/60000 (57%)]\tLoss: 0.016156\n",
      "Train Epoch: Model Number: 1 0 [34176/60000 (57%)]\tLoss: 0.019213\n",
      "Train Epoch: Model Number: 1 0 [34304/60000 (57%)]\tLoss: 0.023936\n",
      "Train Epoch: Model Number: 1 0 [34432/60000 (57%)]\tLoss: 0.013135\n",
      "Train Epoch: Model Number: 1 0 [34560/60000 (58%)]\tLoss: 0.014737\n",
      "Train Epoch: Model Number: 1 0 [34688/60000 (58%)]\tLoss: 0.020710\n",
      "Train Epoch: Model Number: 1 0 [34816/60000 (58%)]\tLoss: 0.016236\n",
      "Train Epoch: Model Number: 1 0 [34944/60000 (58%)]\tLoss: 0.020834\n",
      "Train Epoch: Model Number: 1 0 [35072/60000 (58%)]\tLoss: 0.016220\n",
      "Train Epoch: Model Number: 1 0 [35200/60000 (59%)]\tLoss: 0.019407\n",
      "Train Epoch: Model Number: 1 0 [35328/60000 (59%)]\tLoss: 0.022421\n",
      "Train Epoch: Model Number: 1 0 [35456/60000 (59%)]\tLoss: 0.017861\n",
      "Train Epoch: Model Number: 1 0 [35584/60000 (59%)]\tLoss: 0.017664\n",
      "Train Epoch: Model Number: 1 0 [35712/60000 (59%)]\tLoss: 0.023929\n",
      "Train Epoch: Model Number: 1 0 [35840/60000 (60%)]\tLoss: 0.017842\n",
      "Train Epoch: Model Number: 1 0 [35968/60000 (60%)]\tLoss: 0.016178\n",
      "Train Epoch: Model Number: 1 0 [36096/60000 (60%)]\tLoss: 0.020898\n",
      "Train Epoch: Model Number: 1 0 [36224/60000 (60%)]\tLoss: 0.016233\n",
      "Train Epoch: Model Number: 1 0 [36352/60000 (61%)]\tLoss: 0.019359\n",
      "Train Epoch: Model Number: 1 0 [36480/60000 (61%)]\tLoss: 0.022429\n",
      "Train Epoch: Model Number: 1 0 [36608/60000 (61%)]\tLoss: 0.016298\n",
      "Train Epoch: Model Number: 1 0 [36736/60000 (61%)]\tLoss: 0.020840\n",
      "Train Epoch: Model Number: 1 0 [36864/60000 (61%)]\tLoss: 0.020418\n",
      "Train Epoch: Model Number: 1 0 [36992/60000 (62%)]\tLoss: 0.019390\n",
      "Train Epoch: Model Number: 1 0 [37120/60000 (62%)]\tLoss: 0.016137\n",
      "Train Epoch: Model Number: 1 0 [37248/60000 (62%)]\tLoss: 0.022429\n",
      "Train Epoch: Model Number: 1 0 [37376/60000 (62%)]\tLoss: 0.017671\n",
      "Train Epoch: Model Number: 1 0 [37504/60000 (62%)]\tLoss: 0.022437\n",
      "Train Epoch: Model Number: 1 0 [37632/60000 (63%)]\tLoss: 0.014764\n",
      "Train Epoch: Model Number: 1 0 [37760/60000 (63%)]\tLoss: 0.023792\n",
      "Train Epoch: Model Number: 1 0 [37888/60000 (63%)]\tLoss: 0.027156\n",
      "Train Epoch: Model Number: 1 0 [38016/60000 (63%)]\tLoss: 0.025246\n",
      "Train Epoch: Model Number: 1 0 [38144/60000 (64%)]\tLoss: 0.020787\n",
      "Train Epoch: Model Number: 1 0 [38272/60000 (64%)]\tLoss: 0.020932\n",
      "Train Epoch: Model Number: 1 0 [38400/60000 (64%)]\tLoss: 0.020627\n",
      "Train Epoch: Model Number: 1 0 [38528/60000 (64%)]\tLoss: 0.016323\n",
      "Train Epoch: Model Number: 1 0 [38656/60000 (64%)]\tLoss: 0.016353\n",
      "Train Epoch: Model Number: 1 0 [38784/60000 (65%)]\tLoss: 0.014683\n",
      "Train Epoch: Model Number: 1 0 [38912/60000 (65%)]\tLoss: 0.015655\n",
      "Train Epoch: Model Number: 1 0 [39040/60000 (65%)]\tLoss: 0.020960\n",
      "Train Epoch: Model Number: 1 0 [39168/60000 (65%)]\tLoss: 0.014685\n",
      "Train Epoch: Model Number: 1 0 [39296/60000 (65%)]\tLoss: 0.016612\n",
      "Train Epoch: Model Number: 1 0 [39424/60000 (66%)]\tLoss: 0.020710\n",
      "Train Epoch: Model Number: 1 0 [39552/60000 (66%)]\tLoss: 0.020837\n",
      "Train Epoch: Model Number: 1 0 [39680/60000 (66%)]\tLoss: 0.019414\n",
      "Train Epoch: Model Number: 1 0 [39808/60000 (66%)]\tLoss: 0.024016\n",
      "Train Epoch: Model Number: 1 0 [39936/60000 (67%)]\tLoss: 0.018521\n",
      "Train Epoch: Model Number: 1 0 [40064/60000 (67%)]\tLoss: 0.017813\n",
      "Train Epoch: Model Number: 1 0 [40192/60000 (67%)]\tLoss: 0.014726\n",
      "Train Epoch: Model Number: 1 0 [40320/60000 (67%)]\tLoss: 0.017721\n",
      "Train Epoch: Model Number: 1 0 [40448/60000 (67%)]\tLoss: 0.019144\n",
      "Train Epoch: Model Number: 1 0 [40576/60000 (68%)]\tLoss: 0.015420\n",
      "Train Epoch: Model Number: 1 0 [40704/60000 (68%)]\tLoss: 0.019356\n",
      "Train Epoch: Model Number: 1 0 [40832/60000 (68%)]\tLoss: 0.022423\n",
      "Train Epoch: Model Number: 1 0 [40960/60000 (68%)]\tLoss: 0.017222\n",
      "Train Epoch: Model Number: 1 0 [41088/60000 (68%)]\tLoss: 0.021039\n",
      "Train Epoch: Model Number: 1 0 [41216/60000 (69%)]\tLoss: 0.025469\n",
      "Train Epoch: Model Number: 1 0 [41344/60000 (69%)]\tLoss: 0.025393\n",
      "Train Epoch: Model Number: 1 0 [41472/60000 (69%)]\tLoss: 0.022117\n",
      "Train Epoch: Model Number: 1 0 [41600/60000 (69%)]\tLoss: 0.016267\n",
      "Train Epoch: Model Number: 1 0 [41728/60000 (70%)]\tLoss: 0.016207\n",
      "Train Epoch: Model Number: 1 0 [41856/60000 (70%)]\tLoss: 0.018420\n",
      "Train Epoch: Model Number: 1 0 [41984/60000 (70%)]\tLoss: 0.023936\n",
      "Train Epoch: Model Number: 1 0 [42112/60000 (70%)]\tLoss: 0.020296\n",
      "Train Epoch: Model Number: 1 0 [42240/60000 (70%)]\tLoss: 0.019627\n",
      "Train Epoch: Model Number: 1 0 [42368/60000 (71%)]\tLoss: 0.011646\n",
      "Train Epoch: Model Number: 1 0 [42496/60000 (71%)]\tLoss: 0.011618\n",
      "Train Epoch: Model Number: 1 0 [42624/60000 (71%)]\tLoss: 0.020820\n",
      "Train Epoch: Model Number: 1 0 [42752/60000 (71%)]\tLoss: 0.020769\n",
      "Train Epoch: Model Number: 1 0 [42880/60000 (71%)]\tLoss: 0.023909\n",
      "Train Epoch: Model Number: 1 0 [43008/60000 (72%)]\tLoss: 0.017807\n",
      "Train Epoch: Model Number: 1 0 [43136/60000 (72%)]\tLoss: 0.017778\n",
      "Train Epoch: Model Number: 1 0 [43264/60000 (72%)]\tLoss: 0.013194\n",
      "Train Epoch: Model Number: 1 0 [43392/60000 (72%)]\tLoss: 0.022441\n",
      "Train Epoch: Model Number: 1 0 [43520/60000 (72%)]\tLoss: 0.025251\n",
      "Train Epoch: Model Number: 1 0 [43648/60000 (73%)]\tLoss: 0.020910\n",
      "Train Epoch: Model Number: 1 0 [43776/60000 (73%)]\tLoss: 0.019286\n",
      "Train Epoch: Model Number: 1 0 [43904/60000 (73%)]\tLoss: 0.019130\n",
      "Train Epoch: Model Number: 1 0 [44032/60000 (73%)]\tLoss: 0.013136\n",
      "Train Epoch: Model Number: 1 0 [44160/60000 (74%)]\tLoss: 0.022342\n",
      "Train Epoch: Model Number: 1 0 [44288/60000 (74%)]\tLoss: 0.019435\n",
      "Train Epoch: Model Number: 1 0 [44416/60000 (74%)]\tLoss: 0.021002\n",
      "Train Epoch: Model Number: 1 0 [44544/60000 (74%)]\tLoss: 0.020964\n",
      "Train Epoch: Model Number: 1 0 [44672/60000 (74%)]\tLoss: 0.020803\n",
      "Train Epoch: Model Number: 1 0 [44800/60000 (75%)]\tLoss: 0.022506\n",
      "Train Epoch: Model Number: 1 0 [44928/60000 (75%)]\tLoss: 0.013089\n",
      "Train Epoch: Model Number: 1 0 [45056/60000 (75%)]\tLoss: 0.020790\n",
      "Train Epoch: Model Number: 1 0 [45184/60000 (75%)]\tLoss: 0.019280\n",
      "Train Epoch: Model Number: 1 0 [45312/60000 (75%)]\tLoss: 0.022815\n",
      "Train Epoch: Model Number: 1 0 [45440/60000 (76%)]\tLoss: 0.017707\n",
      "Train Epoch: Model Number: 1 0 [45568/60000 (76%)]\tLoss: 0.016200\n",
      "Train Epoch: Model Number: 1 0 [45696/60000 (76%)]\tLoss: 0.022156\n",
      "Train Epoch: Model Number: 1 0 [45824/60000 (76%)]\tLoss: 0.019394\n",
      "Train Epoch: Model Number: 1 0 [45952/60000 (77%)]\tLoss: 0.014643\n",
      "Train Epoch: Model Number: 1 0 [46080/60000 (77%)]\tLoss: 0.017832\n",
      "Train Epoch: Model Number: 1 0 [46208/60000 (77%)]\tLoss: 0.023963\n",
      "Train Epoch: Model Number: 1 0 [46336/60000 (77%)]\tLoss: 0.019113\n",
      "Train Epoch: Model Number: 1 0 [46464/60000 (77%)]\tLoss: 0.025524\n",
      "Train Epoch: Model Number: 1 0 [46592/60000 (78%)]\tLoss: 0.014738\n",
      "Train Epoch: Model Number: 1 0 [46720/60000 (78%)]\tLoss: 0.019318\n",
      "Train Epoch: Model Number: 1 0 [46848/60000 (78%)]\tLoss: 0.023837\n",
      "Train Epoch: Model Number: 1 0 [46976/60000 (78%)]\tLoss: 0.017941\n",
      "Train Epoch: Model Number: 1 0 [47104/60000 (78%)]\tLoss: 0.025422\n",
      "Train Epoch: Model Number: 1 0 [47232/60000 (79%)]\tLoss: 0.020865\n",
      "Train Epoch: Model Number: 1 0 [47360/60000 (79%)]\tLoss: 0.023735\n",
      "Train Epoch: Model Number: 1 0 [47488/60000 (79%)]\tLoss: 0.017905\n",
      "Train Epoch: Model Number: 1 0 [47616/60000 (79%)]\tLoss: 0.011619\n",
      "Train Epoch: Model Number: 1 0 [47744/60000 (80%)]\tLoss: 0.019980\n",
      "Train Epoch: Model Number: 1 0 [47872/60000 (80%)]\tLoss: 0.019214\n",
      "Train Epoch: Model Number: 1 0 [48000/60000 (80%)]\tLoss: 0.019213\n",
      "Train Epoch: Model Number: 1 0 [48128/60000 (80%)]\tLoss: 0.013825\n",
      "Train Epoch: Model Number: 1 0 [48256/60000 (80%)]\tLoss: 0.022361\n",
      "Train Epoch: Model Number: 1 0 [48384/60000 (81%)]\tLoss: 0.011648\n",
      "Train Epoch: Model Number: 1 0 [48512/60000 (81%)]\tLoss: 0.022337\n",
      "Train Epoch: Model Number: 1 0 [48640/60000 (81%)]\tLoss: 0.020676\n",
      "Train Epoch: Model Number: 1 0 [48768/60000 (81%)]\tLoss: 0.022333\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-6caa99b6db02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_of_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_exp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoosting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-11163a2756f9>\u001b[0m in \u001b[0;36mGradientBoosting\u001b[1;34m(initial_model, M)\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhoutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                 \u001b[0mHoptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m                 \u001b[0mHoptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;31m# Print out current Process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_models = 3\n",
    "models, gamma_exp = GradientBoosting(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "f found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n",
      "WARNING:root:NaN or Inf found in input tensor.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2733d602f048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_of_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodels_adp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma_exp_adp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mADPGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_of_models\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-33847f3a944f>\u001b[0m in \u001b[0;36mADPGradient\u001b[1;34m(initial_model, M)\u001b[0m\n\u001b[0;32m     65\u001b[0m                             \u001b[0mmask_non_y_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasked_select\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool_R_y_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_digits\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                             \u001b[0mensemble_probs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgamma_temp\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_of_models = 3\n",
    "models_adp, gamma_exp_adp = ADPGradient(initial_model, num_of_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the models trained\n",
    "for i in range(num_of_models):\n",
    "    model = models[i]\n",
    "    torch.save(model, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(i) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the optimized gamma\n",
    "torch.save(gamma_exp, 'C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models from the local files\n",
    "num_of_models = 3\n",
    "models = []\n",
    "for x in range(num_of_models):\n",
    "    globals()['model%s' % x] = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/model' + str(1) + '.pth')\n",
    "    models.append(globals()['model%s' % x])\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimized gamma from the local files\n",
    "gamma_exp = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/gamma_exp.txt')\n",
    "print(gamma_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the accuracy of the ensemble model\n",
    "# initial_model = torch.load('C:/Users/cozyn/OneDrive/Desktop/Research/Adversarial-Machine-Learning/results/initial_model.pth')\n",
    "initial_model.eval()\n",
    "\n",
    "loss = 0\n",
    "correct = 0\n",
    "    \n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = initial_model(data)\n",
    "        for i in range(num_of_models):\n",
    "            model = models[i]\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                output = output.cuda()\n",
    "                gamma_temp = gamma_exp[i]\n",
    "                gamma_temp = gamma_temp.cuda()\n",
    "            output = output + gamma_temp * model(data)\n",
    "        loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "loss /= len(train_loader.dataset)  \n",
    "print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "    loss, correct, len(train_loader.dataset),\n",
    "    100. * correct / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attack the initial model using CW attack\n",
    "initial_model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW(initial_model, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble CW attack\n",
    "class Attack_ensemble(object):\n",
    "    r\"\"\"\n",
    "    Base class for all attacks.\n",
    "    .. note::\n",
    "        It automatically set device to the device where given model is.\n",
    "        It temporarily changes the original model's training mode to `test`\n",
    "        by `.eval()` only during an attack process.\n",
    "    \"\"\"\n",
    "    def __init__(self, name, model, models, gamma):\n",
    "        r\"\"\"\n",
    "        Initializes internal attack state.\n",
    "        Arguments:\n",
    "            name (str) : name of an attack.\n",
    "            model (torch.nn.Module): model to attack.\n",
    "        \"\"\"\n",
    "\n",
    "        self.attack = name\n",
    "        self.model = model\n",
    "        self.models = models\n",
    "        self.gamma = gamma\n",
    "        self.model_name = str(model).split(\"(\")[0]\n",
    "\n",
    "        self.training = model.training\n",
    "        self.device = next(model.parameters()).device\n",
    "        \n",
    "        self._targeted = 1\n",
    "        self._attack_mode = 'original'\n",
    "        self._return_type = 'float'\n",
    "\n",
    "    def forward(self, *input):\n",
    "        r\"\"\"\n",
    "        It defines the computation performed at every call.\n",
    "        Should be overridden by all subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_attack_mode(self, mode):\n",
    "        r\"\"\"\n",
    "        Set the attack mode.\n",
    "  \n",
    "        Arguments:\n",
    "            mode (str) : 'original' (DEFAULT)\n",
    "                         'targeted' - Use input labels as targeted labels.\n",
    "                         'least_likely' - Use least likely labels as targeted labels.\n",
    "        \"\"\"\n",
    "        if self._attack_mode is 'only_original':\n",
    "            raise ValueError(\"Changing attack mode is not supported in this attack method.\")\n",
    "            \n",
    "        if mode==\"original\":\n",
    "            self._attack_mode = \"original\"\n",
    "            self._targeted = 1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"targeted\":\n",
    "            self._attack_mode = \"targeted\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_label\n",
    "        elif mode==\"least_likely\":\n",
    "            self._attack_mode = \"least_likely\"\n",
    "            self._targeted = -1\n",
    "            self._transform_label = self._get_least_likely_label\n",
    "        else:\n",
    "            raise ValueError(mode + \" is not a valid mode. [Options : original, targeted, least_likely]\")\n",
    "            \n",
    "    def set_return_type(self, type):\n",
    "        r\"\"\"\n",
    "        Set the return type of adversarial images: `int` or `float`.\n",
    "        Arguments:\n",
    "            type (str) : 'float' or 'int'. (DEFAULT : 'float')\n",
    "        \"\"\"\n",
    "        if type == 'float':\n",
    "            self._return_type = 'float'\n",
    "        elif type == 'int':\n",
    "            self._return_type = 'int'\n",
    "        else:\n",
    "            raise ValueError(type + \" is not a valid type. [Options : float, int]\")\n",
    "\n",
    "    def save(self, save_path, data_loader, verbose=True):\n",
    "        r\"\"\"\n",
    "        Save adversarial images as torch.tensor from given torch.utils.data.DataLoader.\n",
    "        Arguments:\n",
    "            save_path (str) : save_path.\n",
    "            data_loader (torch.utils.data.DataLoader) : data loader.\n",
    "            verbose (bool) : True for displaying detailed information. (DEFAULT : True)\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        image_list = []\n",
    "        label_list = []\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        total_batch = len(data_loader)\n",
    "\n",
    "        for step, (images, labels) in enumerate(data_loader):\n",
    "            adv_images = self.__call__(images, labels)\n",
    "\n",
    "            image_list.append(adv_images.cpu())\n",
    "            label_list.append(labels.cpu())\n",
    "\n",
    "            if self._return_type == 'int':\n",
    "                adv_images = adv_images.float()/255\n",
    "\n",
    "            if verbose:\n",
    "                outputs = self.model(adv_images)\n",
    "                for i in range(len(self.models)):\n",
    "                    sub_model = self.models[i]\n",
    "                    outputs = outputs + self.gamma[i] * sub_model(adv_images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels.to(self.device)).sum()\n",
    "\n",
    "                acc = 100 * float(correct) / total\n",
    "                print('- Save Progress : %2.2f %% / Accuracy : %2.2f %%' % ((step+1)/total_batch*100, acc), end='\\r')\n",
    "\n",
    "        x = torch.cat(image_list, 0)\n",
    "        y = torch.cat(label_list, 0)\n",
    "        torch.save((x, y), save_path)\n",
    "        print('\\n- Save Complete!')\n",
    "\n",
    "        self._switch_model()\n",
    "        \n",
    "    def _transform_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "        \n",
    "    def _get_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return input labels.\n",
    "        \"\"\"\n",
    "        return labels\n",
    "    \n",
    "    def _get_least_likely_label(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Function for changing the attack mode.\n",
    "        Return least likely labels.\n",
    "        \"\"\"\n",
    "        outputs = self.model(images)\n",
    "        for i in range(len(self.models)):\n",
    "            sub_model = self.models[i]\n",
    "            outputs = outputs + self.gamma[i] * sub_model(images)\n",
    "        _, labels = torch.min(outputs.data, 1)\n",
    "        labels = labels.detach_()\n",
    "        return labels\n",
    "    \n",
    "    def _to_uint(self, images):\n",
    "        r\"\"\"\n",
    "        Function for changing the return type.\n",
    "        Return images as int.\n",
    "        \"\"\"\n",
    "        return (images*255).type(torch.uint8)\n",
    "\n",
    "    def _switch_model(self):\n",
    "        r\"\"\"\n",
    "        Function for changing the training mode of the model.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            self.model.train()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            for i in range(len(self.models)):\n",
    "                self.models[i].eval()\n",
    "\n",
    "    def __str__(self):\n",
    "        info = self.__dict__.copy()\n",
    "        \n",
    "        del_keys = ['model', 'attack']\n",
    "        \n",
    "        for key in info.keys():\n",
    "            if key[0] == \"_\" :\n",
    "                del_keys.append(key)\n",
    "                \n",
    "        for key in del_keys:\n",
    "            del info[key]\n",
    "        \n",
    "        info['attack_mode'] = self._attack_mode\n",
    "        if info['attack_mode'] == 'only_original' :\n",
    "            info['attack_mode'] = 'original'\n",
    "            \n",
    "        info['return_type'] = self._return_type\n",
    "        \n",
    "        return self.attack + \"(\" + ', '.join('{}={}'.format(key, val) for key, val in info.items()) + \")\"\n",
    "\n",
    "    def __call__(self, *input, **kwargs):\n",
    "        self.model.eval()\n",
    "        for i in range(len(self.models)):\n",
    "            self.models[i].eval()\n",
    "        images = self.forward(*input, **kwargs)\n",
    "        self._switch_model()\n",
    "\n",
    "        if self._return_type == 'int':\n",
    "            images = self._to_uint(images)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Ensemble CW Attack\n",
    "import warnings\n",
    "\n",
    "class CW_Ensemble(Attack_ensemble):\n",
    "    r\"\"\"\n",
    "    CW in the paper 'Towards Evaluating the Robustness of Neural Networks'\n",
    "    [https://arxiv.org/abs/1608.04644]\n",
    "    Distance Measure : L2\n",
    "        \n",
    "    Arguments:\n",
    "        model (nn.Module): model to attack.\n",
    "        c (float): c in the paper. parameter for box-constraint. (DEFALUT : 1e-4)    \n",
    "            :math:`minimize \\Vert\\frac{1}{2}(tanh(w)+1)-x\\Vert^2_2+c\\cdot f(\\frac{1}{2}(tanh(w)+1))`    \n",
    "        kappa (float): kappa (also written as 'confidence') in the paper. (DEFALUT : 0)\n",
    "            :math:`f(x')=max(max\\{Z(x')_i:i\\neq t\\} -Z(x')_t, - \\kappa)`\n",
    "        steps (int): number of steps. (DEFALUT : 1000)\n",
    "        lr (float): learning rate of the Adam optimizer. (DEFALUT : 0.01)\n",
    "        \n",
    "    .. warning:: With default c, you can't easily get adversarial images. Set higher c like 1.\n",
    "    \n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`,        `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "          \n",
    "    Examples::\n",
    "        >>> attack = torchattacks.CW(model, targeted=False, c=1e-4, kappa=0, steps=1000, lr=0.01)\n",
    "        >>> adv_images = attack(images, labels)\n",
    "        \n",
    "    .. note:: NOT IMPLEMENTED methods in the paper due to time consuming.\n",
    "    \n",
    "        (1) Binary search for c.\n",
    "        \n",
    "        (2) Choosing best L2 adversaries.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, models, gamma, c=1e-4, kappa=0, steps=1000, lr=0.01):\n",
    "        super(CW_Ensemble, self).__init__(\"CW\", model, models, gamma)\n",
    "        self.c = c\n",
    "        self.kappa = kappa\n",
    "        self.steps = steps\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        images = images.to(self.device)\n",
    "        labels = labels.to(self.device)\n",
    "        labels = self._transform_label(images, labels)\n",
    "\n",
    "        # f-function in the paper\n",
    "        def f(x):\n",
    "            outputs = self.model(x)\n",
    "            for i in range(len(self.models)):\n",
    "                sub_model = self.models[i]\n",
    "                outputs = outputs + self.gamma[i] * sub_model(x)\n",
    "            one_hot_labels = torch.eye(len(outputs[0]))[labels].to(self.device)\n",
    "\n",
    "            i, _ = torch.max((1-one_hot_labels)*outputs, dim=1)\n",
    "            j = torch.masked_select(outputs, one_hot_labels.bool())\n",
    "\n",
    "            return torch.clamp(self._targeted*(j-i), min=-self.kappa)\n",
    "\n",
    "        w = torch.zeros_like(images).to(self.device)\n",
    "        w.detach_()\n",
    "        w.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam([w], lr=self.lr)\n",
    "        prev = 1e10\n",
    "\n",
    "        for step in range(self.steps):\n",
    "\n",
    "            a = 1/2*(nn.Tanh()(w) + 1)\n",
    "\n",
    "            loss1 = nn.MSELoss(reduction='sum')(a, images)\n",
    "            loss2 = torch.sum(self.c*f(a))\n",
    "\n",
    "            cost = loss1 + loss2\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            cost.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            # Early Stop when loss does not converge.\n",
    "            if step % (self.steps//10) == 0:\n",
    "                if cost > prev:\n",
    "                    warnings.warn(\"Early stopped because the loss is not converged.\")\n",
    "                    return (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "                prev = cost\n",
    "\n",
    "            # print('- CW Attack Progress : %2.2f %%        ' %((step+1)/self.steps*100), end='\\r')\n",
    "\n",
    "        adv_images = (1/2*(nn.Tanh()(w) + 1)).detach()\n",
    "\n",
    "        return adv_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attack the ensemble model\n",
    "initial_model.eval()\n",
    "for i in range(num_of_models):\n",
    "    models[i].eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "cw_attack = CW_Ensemble(model = initial_model,models = models,gamma = gamma_exp, c=1)\n",
    "\n",
    "for data, target in test_loader:\n",
    "\n",
    "        images = cw_attack(data, target).cuda()\n",
    "        outputs = initial_model(images)\n",
    "        for i in range(num_of_models):\n",
    "            sub_model = models[i]\n",
    "            outputs = outputs + gamma_exp[i] * sub_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target.cuda()).sum()\n",
    "    \n",
    "print('Robust accuracy: %.2f %%' % (100 * float(correct) / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}